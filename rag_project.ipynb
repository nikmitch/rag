{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting hands-on experience with LLMs\n",
    "\n",
    "It seems like it will plausibly be valuable to be able to run LLMs locally on my laptop, or being able to hook into them for parts of tasks.\n",
    "\n",
    "Here's the rough kind of idea of what I want to learn in this project\n",
    "\n",
    "- Learn how to download, install and interact with a LLM (Llama3) hosted locally on my computer, sending it text directly and asking it simple questions\n",
    "- Learn how to interact with ChatGPT via an API, so I can do more automation and use it in coding projects\n",
    "- Understand environments better (through the course of debugging all this stuff) -- added post hoc lol - I screwed up my base conda installation when I was trying to follow one of the videos at the start of this process\n",
    "- Understand embeddings better, and build skills in visualisation to illustrate the distance between different words\n",
    "- Make a basic RAG that can read a larger document and answer basic questions from the text (from a file in .md or .txt format) - following [this video](https://www.youtube.com/watch?v=tcqEUSNCn8I). \n",
    " - Tried out both openai embeddings and ollama embeddings \n",
    "    - <span style=\"color:red\">Got negative similarity scores when doing a retrieval search from the database with ollama embeddings, which shouldn't be a thing (Should be between zero and 1)!. I found this [git issue](https://github.com/langchain-ai/langchain/issues/10864) logged on the langchain github with people mentioning this issue with a bunch of models (including 1 or 2 talking about getting the issue with locall llama in the last week). It does not yet appear to be resolved. Going to stick with OpenAI embeddings for now, and maybe check back in a while and see if anyone has a solution.</span>\n",
    "    - [This video](https://www.youtube.com/watch?v=2TJxpyO3ei4) might also help for the ollama version once the negative distance issue is resolved\n",
    "- extend to be able to read pdfs or arbitrary filetypes, using [this video](https://www.youtube.com/watch?v=2TJxpyO3ei4) then maybe [this video](https://www.youtube.com/watch?v=svzd5d1LXGk) -- or maybe another one entirely. [This documentation](https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/pdf/) might also be helpful\n",
    "\n",
    "[this link](https://github.com/langchain-ai/langchain/issues/14872) might help if I get Chroma readonly issues again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with ChatGPT via API\n",
    "\n",
    "This feels like it will be useful in a bunch of different projects. I've done some exploration of this (chat completion, embedding, image generation and text-to-speech) in `openai-test.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with Llama3.1 (no embedding)\n",
    "This next part is just me trying to interact with the model directly and feeding it a text file (no embedding etc), to see whether it'll provide sensible responses. \n",
    "I have already downloaded llama3.1 by downloading and installing Ollama and then running `ollama run llama3.1' in the terminal. I'm using the 4GB version because the 40GB version is way way too slow on my laptop (basically doesn't run). \n",
    "\n",
    "I've created my own version of the model with the temperature paramater set to zero (to make it completely deterministic and hence reproducible). I did this using the Modelfile in this directory by running `ollama create llama3.1_T0 -f ./Modelfile`.\n",
    "\n",
    "Now I want to see if I can interact with it with the llama package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "# note -- extremely bizarre that \"import ollama\" failed\n",
    "# after a successful-looking \"conda install ollama\" and required \n",
    "# me to \"pip install ollama\" in order to work??\n",
    "import os # will need this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I noticed that llama tends to print really long lines so I need to scroll sideways. I'm not enjoying that, so I'm making\n",
    "# a wrapped print function to fix it\n",
    "\n",
    "import textwrap\n",
    "\n",
    "def wprint(text, width = 120):\n",
    "  wrapped_text = textwrap.fill(text, width=width)\n",
    "  print(wrapped_text)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've downloaded a transcript of a YouTube video essay about how sound design is used in the recent Batman movie (see `data/personal/batman_sound_video_essay.txt`), and am prompting the model to answer basic questions about it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining function to take a question input and answer it with information from a specified file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_read_and_respond(input_file, question, print_prompt_with_data = False):\n",
    "    with open(input_file,'r') as file:\n",
    "        data = file.read()\n",
    "\n",
    "\n",
    "    #debugging statement to confirm file was loaded\n",
    "    if data:\n",
    "        print(\"File loaded successfully\")\n",
    "    else:\n",
    "        print(\"Load in a file\")\n",
    "\n",
    "\n",
    "    prompt_01 = f\"{data} #### From this text, {question}\"\n",
    "\n",
    "    if print_prompt_with_data:\n",
    "        wprint(\"Prompt: \"+prompt_01)\n",
    "\n",
    "    print(\"Generating a response: \")\n",
    "\n",
    "\n",
    "\n",
    "    response = ollama.chat(model = 'llama3.1_TO', #this is a version of the model with the temperature set to zero so it's fully deterministic (and thus reproducible)\n",
    "                            messages = [{\n",
    "                            \"role\":\"user\",\n",
    "                            #    \"content\":\"tell me about a cool species of frog\"\n",
    "                            \"content\": prompt_01\n",
    "                        }])\n",
    "\n",
    "    wprint(response[\"message\"][\"content\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions from Batman video essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully\n",
      "Generating a response: \n",
      "In the movie, the sound of rain is initially audible, but once gunfire breaks out, it \"falls away into silence\". The\n",
      "sound designer used a technique where they left the volume of the rain sound as it was before the gunshots occurred,\n",
      "which made the gunshots themselves sound quieter than they actually were. This was done to create an impressionistic and\n",
      "expressionist sound effect that emphasized the intensity of the scene rather than a realistic representation of the\n",
      "actual sounds.\n"
     ]
    }
   ],
   "source": [
    "llama_read_and_respond(input_file='data/personal/batman_sound_video_essay.txt', \n",
    "                       question = 'tell me about how the sound of rain is used in the movie')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully\n",
      "Generating a response: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the text, the sponsor of the video is Nebula, a streaming platform. The video also mentions a \"Curiosity\n",
      "Stream\" bundle offer, which seems to be related to Nebula as well.\n"
     ]
    }
   ],
   "source": [
    "llama_read_and_respond(input_file='data/personal/batman_sound_video_essay.txt', \n",
    "                       question = 'who sponsored the video?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this seems to be working. Now I'd like it to try reading something from my CV, because it seemed to be struggling with that when I was running it from the terminal. I've just changed the extension from a .tex file to .txt, and I want to see if it can answer basic questions (e.g. about dates of employment). This might be harder for it to do because it's still got all of these latex formatting things in there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions from CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully\n",
      "Generating a response: \n",
      "According to the text, the most recent job listed is:  **NZ Royal Commission Inquiry - COVID-19 Lessons Learned**\n",
      "**Principal Data Analyst (May 2024 -- July 2024)**  In this role, you created high-quality visualizations to support the\n",
      "Inquiry, including:  1. Visualizations that contextualized pandemic trends (COVID-19 cases, hospitalizations, deaths,\n",
      "and vaccinations) in New Zealand against policy decisions (e.g., lockdowns, border closures) and pandemic trends in\n",
      "other countries. 2. Analyses and visualizations highlighting the disparate impact of COVID-19 on Māori and Pacific\n",
      "ethnic groups and people living in areas of higher socioeconomic deprivation. 3. Worked closely with the Chair of the\n",
      "Commission to discuss how to tell the story of the COVID pandemic through these visualizations, drawing out lessons for\n",
      "future pandemics.\n"
     ]
    }
   ],
   "source": [
    "llama_read_and_respond(input_file='data/personal/Nik_Mitchell_CV_2024_07_21.txt', \n",
    "                       question = 'what is the most recent job on that list, and what did I do in that job?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the correct answer but it seems to really directly copy-paste exactly what I wrote in my bullet points here. Next, asking it to be more concise & summarise a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully\n",
      "Generating a response: \n",
      "The most recent job listed is \"Principal Data Analyst\" at the NZ Royal Commission Inquiry - COVID-19 Lessons Learned,\n",
      "which took place from May 2024 to July 2024.  In this role, I was responsible for:  * Creating high-quality\n",
      "visualizations to support the inquiry * Conducting analyses and creating visualizations to highlight pandemic trends and\n",
      "disparities in Māori and Pacific ethnic groups and people living in areas of higher socioeconomic deprivation * Working\n",
      "closely with the Chair of the Commission to discuss how to tell the story of the COVID-19 pandemic through\n",
      "visualizations  Please note that this summary is based on the provided LaTeX code, which may not accurately reflect my\n",
      "actual responsibilities or experiences.\n"
     ]
    }
   ],
   "source": [
    "llama_read_and_respond(input_file='data/personal/Nik_Mitchell_CV_2024_07_21.txt', \n",
    "                       question = 'what is the most recent job on that list, and what did I do in that job? Please be concise and summarise the responsibilities rather than copying the whole description')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has shaved off a few words without changing the meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (Retrieval-Augmented Generator)\n",
    "\n",
    "Why would we want to create a RAG? The above seemed to work just fine.\n",
    "\n",
    "I have a suspicion that the issue here is to do with context windows. When making a RAG, we're first going to create a database by chunking up all the inputs into manageable-sized pieces (with overlap between chunks) and then using particular embeddings to encode the meaning of the chunks as vectors. Once we have that, we can use the same embeddings on the input question, and then retrieve the top few chunks that have the most similar meaning vectors (e.g. smallest euclidean distance apart) and use this subset of data to construct the answer from.\n",
    "\n",
    "I suspect that the reason for creating a RAG is this is a context window limitation. The LLM needs to know which information to focus on, so having a method for retrieving the most relevant data allows it to work much more efficiently with a large amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now working through [this video](https://www.youtube.com/watch?v=tcqEUSNCn8I)(about how to make a RAG) - will use OpenAI embeddings here rather than Llama.\n",
    "\n",
    "Has an associated [git repo](https://github.com/pixegami/langchain-rag-tutorial) - might clone this.\n",
    "\n",
    "I've grabbed a version of the Wizard of Oz from the Gutenberg Project website [link](https://www.gutenberg.org/ebooks/55)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning investigation\n",
    "\n",
    "I'm kinda curious to try to build something a bit more flexible here, and use that to investigate a few questions\n",
    "- Does it make a difference if you use OpenAIEmbeddings() or OllamaEmbeddings()?\n",
    "- Can I build several different chromadbs with different embeddings for different datasets\n",
    "    - Wizard of Oz\n",
    "    - Alice in Wonderland\n",
    "    - My personal files (CV, batman video essay)\n",
    "        - does it matter if I mash these together into a single database, even though they're about totally different things?\n",
    "- do you get better performance with bigger chunks?\n",
    "- can I extend this to read PDF files?\n",
    "\n",
    "I'm a bit worried about doing this if it's not on the mainline to being able to do AI safety work, but I also think that just being curious and following my nose and making functions to output different things and label files and folders appropriately in python etc is going to be valuable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import openai \n",
    "# from dotenv import load_dotenv\n",
    "import os\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chroma_path(data_description, embeddings_description):\n",
    "    CHROMA_PATH=os.path.join(\"chroma\",data_description, embeddings_description)\n",
    "    return CHROMA_PATH\n",
    "\n",
    "def get_data_path(data_description):\n",
    "    DATA_PATH =os.path.join(\"data\",data_description)\n",
    "    return DATA_PATH\n",
    "\n",
    "def get_embedding_function(embeddings_description):\n",
    "    if embeddings_description == \"openai_embeddings\":\n",
    "        embedding_function = OpenAIEmbeddings()\n",
    "    elif embeddings_description == \"ollama_embeddings\":\n",
    "        embedding_function = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    else:\n",
    "        print(\"please specify either 'openai_embeddings' or 'ollama_embeddings'\")\n",
    "    return embedding_function\n",
    "\n",
    "\n",
    "\n",
    "def generate_data_store(data_description, embeddings_description):\n",
    "\n",
    "    CHROMA_PATH= get_chroma_path(data_description, embeddings_description)\n",
    "    DATA_PATH =  get_data_path(data_description)\n",
    "\n",
    "    print(f\"Data source: {data_description}, Embeddings: {embeddings_description}\")\n",
    "\n",
    "    # print(f\"CHROMA_PATH is {CHROMA_PATH}\")\n",
    "    # print(f\"DATA_PATH is {DATA_PATH}\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    documents = load_documents(data_path=DATA_PATH)\n",
    "    chunks = split_text(documents)\n",
    "    save_to_chroma(chunks, get_embedding_function(embeddings_description), chroma_path= CHROMA_PATH)\n",
    "\n",
    "\n",
    "def load_documents(data_path):\n",
    "    loader = DirectoryLoader(data_path, glob=\"*.md\")\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "\n",
    "def split_text(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "    ## print test example\n",
    "    # document = chunks[10]\n",
    "    # print(document.page_content)\n",
    "    # print(document.metadata)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def save_to_chroma(chunks: list[Document], embedding_function, chroma_path):\n",
    "    # Clear out the database first.\n",
    "    if os.path.exists(chroma_path):\n",
    "        shutil.rmtree(chroma_path)\n",
    "\n",
    "    # Create a new DB from the documents.\n",
    "    db = Chroma.from_documents(\n",
    "        chunks, embedding_function, persist_directory=chroma_path\n",
    "    )\n",
    "    db.persist()\n",
    "    print(f\"Saved {len(chunks)} chunks to {chroma_path}.\")\n",
    "\n",
    "\n",
    "# generate_data_store(data_description       = data_descriptions[0],\n",
    "#                     embeddings_description = embeddings_descriptions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source: wizard_of_oz, Embeddings: openai_embeddings\n",
      "Split 1 documents into 1127 chunks.\n",
      "Saved 1127 chunks to chroma/wizard_of_oz/openai_embeddings.\n",
      "Data source: wizard_of_oz, Embeddings: ollama_embeddings\n",
      "Split 1 documents into 1127 chunks.\n",
      "Saved 1127 chunks to chroma/wizard_of_oz/ollama_embeddings.\n",
      "Data source: alice_in_wonderland, Embeddings: openai_embeddings\n",
      "Split 1 documents into 801 chunks.\n",
      "Saved 801 chunks to chroma/alice_in_wonderland/openai_embeddings.\n",
      "Data source: alice_in_wonderland, Embeddings: ollama_embeddings\n",
      "Split 1 documents into 801 chunks.\n",
      "Saved 801 chunks to chroma/alice_in_wonderland/ollama_embeddings.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "# data_descriptions = [\"wizard_of_oz\",\"alice_in_wonderland\",\"personal\"] ## Commenting out personal for now because it does't use markdown files\n",
    "data_descriptions = [\"wizard_of_oz\",\"alice_in_wonderland\"]\n",
    "embeddings_descriptions = [\"openai_embeddings\",\"ollama_embeddings\"] #note that ollama has the issue with negative relevance scores\n",
    "# embeddings_descriptions = [\"openai_embeddings\"] # Commenting out ollama for now because it's not working\n",
    "\n",
    "for data_description, embeddings_description in itertools.product(data_descriptions, embeddings_descriptions):\n",
    "    generate_data_store(data_description, embeddings_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, that works. This is exciting. I should get the question-asking part running up soon too.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding investigation\n",
    "\n",
    "I'm also curious now about the embeddings, and how they work for ollama versus openai. So I looked into it in `embeddings_investigation.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering questions\n",
    "\n",
    "Initially we just have the code to do the openai embeddings and chat to openAI. Is it possible to use the OpenAI embeddings and generate the response with Llama3? My guess is yes, but also that the quality of the answers will depend primarily on the quality of the embeddings, since the AI model can't answer correctly if the correct information isn't retrieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to ask several basic question questions to pull info from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import argparse\n",
    "from langchain_community.vectorstores import Chroma\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Define a dictionary to hold lists of questions for each book\n",
    "question_dict = {\n",
    "    \"wizard_of_oz\": [\n",
    "        \"How does Dorothy get back home?\",\n",
    "        \"Who are Alice's companions, and what are they each missing?\",\n",
    "        \"What obstacles do Dorothy and her friends face on their journey?\",\n",
    "    ],\n",
    "    \"alice_in_wonderland\": [\n",
    "        \"How does Alice end up in Wonderland?\",\n",
    "        \"What does the Cheshire cat tell Alice?\",\n",
    "        \"What does Alice do with the Mad Hatter?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def get_query_list(data_description):\n",
    "    \"\"\"\n",
    "    Return a list of questions for the specified book description.\n",
    "    \"\"\"\n",
    "    return question_dict.get(data_description, [])\n",
    "\n",
    "def answer_query_from_database(data_description, embeddings_description, show_source_passages=False):\n",
    "    \"\"\"\n",
    "    Answer queries from the database based on the book description and embeddings description.\n",
    "    Optionally show source passages if show_source_passages is True.\n",
    "    \"\"\"\n",
    "    chroma_path = get_chroma_path(data_description, embeddings_description)\n",
    "    embedding_function = get_embedding_function(embeddings_description)\n",
    "\n",
    "    db = Chroma(\n",
    "        persist_directory=chroma_path,\n",
    "        embedding_function=embedding_function\n",
    "    )\n",
    "    \n",
    "    print(f\"Loading the Chroma database from {chroma_path}, using the {embeddings_description} embedding function.\")\n",
    "\n",
    "    # Get the list of questions for the specified book\n",
    "    query_list = get_query_list(data_description)\n",
    "    \n",
    "    # Initialize an empty string to store responses\n",
    "    all_responses = \"\"\n",
    "\n",
    "    for query_text in query_list:\n",
    "        # Search the DB\n",
    "        results = db.similarity_search_with_relevance_scores(query_text, k=8)\n",
    "        \n",
    "        if len(results) == 0:  # Removed score check\n",
    "        # if len(results) == 0 or results[0][1] < 0.7:\n",
    "            print(f\"Unable to find matching results for the query: {query_text}\")\n",
    "            continue\n",
    "\n",
    "        # Prepare the context text\n",
    "        context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "        prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "        prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "\n",
    "        # print(prompt)\n",
    "\n",
    "        # Get response from the model\n",
    "        model = ChatOpenAI()\n",
    "        response_text = model.predict(prompt, temperature = 1)\n",
    "\n",
    "        # Retrieve sources\n",
    "        if show_source_passages:\n",
    "            sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "            formatted_response = f\"Question: {query_text}\\nResponse: {response_text}\\nSources: {sources}\"\n",
    "        else:\n",
    "            formatted_response = f\"Question: {query_text}\\nResponse: {response_text}\"\n",
    "\n",
    "        # Append the response to the all_responses string\n",
    "        all_responses += formatted_response + \"\\n\\n\\n\"\n",
    "\n",
    "    # Print the final responses\n",
    "    print(all_responses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've set the temperature to zero on the model.predict() function in answer_query_from_database, so if I call the function again it should produce an identical output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wizard of Oz Qs\n",
    "\n",
    "#### OpenAI embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the Chroma database from chroma/wizard_of_oz/openai_embeddings, using the openai_embeddings embedding function.\n",
      "Question: How does Dorothy get back home?\n",
      "Response: Dorothy gets back home by clicking the heels of her shoes together three times and saying \"Take me home to Aunt Em!\" This causes her to whirling through the air and be transported back to Kansas.\n",
      "\n",
      "\n",
      "Question: Who are Alice's companions, and what are they each missing?\n",
      "Response: Dorothy's companions are the Tin Woodman, the Scarecrow, and the Lion. The Tin Woodman is missing a heart, the Scarecrow is missing a brain, and the Lion is missing courage.\n",
      "\n",
      "\n",
      "Question: What obstacles do Dorothy and her friends face on their journey?\n",
      "Response: Dorothy and her friends face obstacles such as a long journey through varying terrain, potential dangers along the way, magical forces trying to harm them, aggressive trees blocking their path, and a rough road with broken or missing yellow bricks.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_description = \"wizard_of_oz\" \n",
    "embeddings_description = \"openai_embeddings\"\n",
    "answer_query_from_database(data_description, embeddings_description, show_source_passages=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ollama embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the Chroma database from chroma/wizard_of_oz/ollama_embeddings, using the ollama_embeddings embedding function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikmitchell/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:784: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 207867}, page_content='Dorothy stood up and found she was in her stocking-feet. For the Silver\\nShoes had fallen off in her flight through the air, and were lost\\nforever in the desert.\\n\\nChapter XXIV\\nHome Again'), -181.71675478893602), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 176421}, page_content='“How can I get to her castle?” asked Dorothy.'), -183.89789263730063), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 207034}, page_content='Dorothy now took Toto up solemnly in her arms, and having said one last\\ngood-bye she clapped the heels of her shoes together three times,\\nsaying:\\n\\n“Take me home to Aunt Em!”\\n\\nInstantly she was whirling through the air, so swiftly that all she\\ncould see or feel was the wind whistling past her ears.'), -190.2610768172678), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 64991}, page_content='the morning; and Dorothy dreamed of the Emerald City, and of the good\\nWizard Oz, who would soon send her back to her own home again.'), -194.1700817028757), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 72930}, page_content='“We must hurry and get back to the road of yellow brick before dark,”\\nhe said; and the Scarecrow agreed with him. So they kept walking until\\nDorothy could stand no longer. Her eyes closed in spite of herself and\\nshe forgot where she was and fell among the poppies, fast asleep.'), -198.5382053591295), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 103265}, page_content='Her friends were sorry, but could do nothing to help her; so Dorothy\\nwent to her own room and lay down on the bed and cried herself to\\nsleep.\\n\\nThe next morning the soldier with the green whiskers came to the\\nScarecrow and said:\\n\\n“Come with me, for Oz has sent for you.”'), -199.42797995351015), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 25454}, page_content='This worried Dorothy a little, but she knew that only the Great Oz\\ncould help her get to Kansas again, so she bravely resolved not to turn\\nback.'), -199.4935806802804), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 87806}, page_content='“And I want him to send me back to Kansas,” said Dorothy.\\n\\n“Where is Kansas?” asked the man, with surprise.\\n\\n“I don’t know,” replied Dorothy sorrowfully, “but it is my home, and\\nI’m sure it’s somewhere.”'), -199.76847793630867)]\n",
      "  warnings.warn(\n",
      "/Users/nikmitchell/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:784: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 175839}, page_content='“Is there no one who can help me?” asked Dorothy earnestly.\\n\\n“Glinda might,” he suggested.\\n\\n“Who is Glinda?” inquired the Scarecrow.'), -272.3784697284533), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 139165}, page_content='“What can I do for my friends?”\\n\\n“We have lost our way,” said Dorothy. “Can you tell us where the\\nEmerald City is?”'), -273.19472403464107), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 69602}, page_content='“Who are you and where are you going?” asked the Stork.\\n\\n“I am Dorothy,” answered the girl, “and these are my friends, the Tin\\nWoodman and the Cowardly Lion; and we are going to the Emerald City.”'), -276.4152881353309), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 47302}, page_content='Dorothy did not say anything, for she was puzzled to know which of her\\ntwo friends was right, and she decided if she could only get back to\\nKansas and Aunt Em, it did not matter so much whether the Woodman had\\nno brains and the Scarecrow no heart, or each got what he wanted.'), -276.9736927427761), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 204958}, page_content='The Scarecrow and the Tin Woodman and the Lion now thanked the Good\\nWitch earnestly for her kindness; and Dorothy exclaimed:\\n\\n“You are certainly as good as you are beautiful! But you have not yet\\ntold me how to get back to Kansas.”'), -280.7614226006048), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 73909}, page_content='“Let us make a chair with our hands and carry her,” said the Scarecrow.\\nSo they picked up Toto and put the dog in Dorothy’s lap, and then they\\nmade a chair with their hands for the seat and their arms for the arms\\nand carried the sleeping girl between them through the flowers.'), -281.6510677198669), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 16415}, page_content='“I am anxious to get back to my aunt and uncle, for I am sure they will\\nworry about me. Can you help me find my way?”\\n\\nThe Munchkins and the Witch first looked at one another, and then at\\nDorothy, and then shook their heads.'), -283.0302366834653), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 23687}, page_content='Dorothy ate a hearty supper and was waited upon by the rich Munchkin\\nhimself, whose name was Boq. Then she sat upon a settee and watched the\\npeople dance.\\n\\nWhen Boq saw her silver shoes he said, “You must be a great sorceress.”\\n\\n“Why?” asked the girl.'), -283.2507328104845)]\n",
      "  warnings.warn(\n",
      "/Users/nikmitchell/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:784: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 18275}, page_content='“How can I get there?” asked Dorothy.\\n\\n“You must walk. It is a long journey, through a country that is\\nsometimes pleasant and sometimes dark and terrible. However, I will use\\nall the magic arts I know of to keep you from harm.”'), -174.35984907351838), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 25454}, page_content='This worried Dorothy a little, but she knew that only the Great Oz\\ncould help her get to Kansas again, so she bravely resolved not to turn\\nback.'), -186.29935397919292), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 137697}, page_content='“If we walk far enough,” said Dorothy, “I am sure we shall sometime\\ncome to some place.”\\n\\nBut day by day passed away, and they still saw nothing before them but\\nthe scarlet fields. The Scarecrow began to grumble a bit.'), -201.54817817937501), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 103265}, page_content='Her friends were sorry, but could do nothing to help her; so Dorothy\\nwent to her own room and lay down on the bed and cried herself to\\nsleep.\\n\\nThe next morning the soldier with the green whiskers came to the\\nScarecrow and said:\\n\\n“Come with me, for Oz has sent for you.”'), -203.79519728393356), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 186725}, page_content='“We must cross this strange place in order to get to the other side,”\\nsaid Dorothy, “for it would be unwise for us to go any other way except\\ndue South.”'), -204.7748491898267), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 69047}, page_content='They walked along as fast as they could, Dorothy only stopping once to\\npick a beautiful flower; and after a time the Tin Woodman cried out:\\n“Look!”\\n\\nThen they all looked at the river and saw the Scarecrow perched upon\\nhis pole in the middle of the water, looking very lonely and sad.'), -204.98218201311897), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 891}, page_content='Introduction\\n Chapter I. The Cyclone\\n Chapter II. The Council with the Munchkins\\n Chapter III. How Dorothy Saved the Scarecrow\\n Chapter IV. The Road Through the Forest\\n Chapter V. The Rescue of the Tin Woodman\\n Chapter VI.  The Cowardly Lion\\n Chapter VII. The Journey to the Great Oz'), -205.00041642565873), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 175627}, page_content='“This little girl,” said the Scarecrow to the soldier, “wishes to cross\\nthe desert. How can she do so?”\\n\\n“I cannot tell,” answered the soldier, “for nobody has ever crossed the\\ndesert, unless it is Oz himself.”\\n\\n“Is there no one who can help me?” asked Dorothy earnestly.'), -205.13776794733388)]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How does Dorothy get back home?\n",
      "Response: Dorothy gets back home by clicking the heels of her shoes together three times and saying, \"Take me home to Aunt Em!\" This instantly whisks her through the air back to her own home in Kansas.\n",
      "\n",
      "\n",
      "Question: Who are Alice's companions, and what are they each missing?\n",
      "Response: Dorothy's companions are the Scarecrow, the Tin Woodman, and the Cowardly Lion. The Scarecrow is missing brains, the Tin Woodman is missing a heart, and the Cowardly Lion is missing courage.\n",
      "\n",
      "\n",
      "Question: What obstacles do Dorothy and her friends face on their journey?\n",
      "Response: Dorothy and her friends face obstacles such as a long journey through a sometimes pleasant and sometimes dark and terrible country, a river they must cross, a desert that has never been crossed before, and the Scarecrow getting stranded in the middle of the water.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_description = \"wizard_of_oz\" \n",
    "embeddings_description = \"ollama_embeddings\"\n",
    "answer_query_from_database(data_description, embeddings_description, show_source_passages=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">NOTE THE WARNINGS ABOUT HOW RELEVANCE SCORES SHOULD BE BETWEEN 0 AND 1</span>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alice in wonderland Qs\n",
    "\n",
    "#### OpenAI embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the Chroma database from chroma/alice_in_wonderland/openai_embeddings, using the openai_embeddings embedding function.\n",
      "Question: How does Alice end up in Wonderland?\n",
      "Response: Alice ends up in Wonderland by following a white rabbit down a rabbit-hole that suddenly drops her down a very deep well.\n",
      "\n",
      "\n",
      "Question: What does the Cheshire cat tell Alice?\n",
      "Response: The Cheshire cat tells Alice that in one direction lives a Hatter and in another direction lives a March Hare, both of whom are mad.\n",
      "\n",
      "\n",
      "Question: What does Alice do with the Mad Hatter?\n",
      "Response: Alice engages in a conversation with the Mad Hatter and the March Hare, listens to their riddles, asks questions, and tries to solve the riddles they pose.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_description = \"alice_in_wonderland\" \n",
    "embeddings_description = \"openai_embeddings\"\n",
    "answer_query_from_database(data_description, embeddings_description, show_source_passages=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ollama embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the Chroma database from chroma/alice_in_wonderland/ollama_embeddings, using the ollama_embeddings embedding function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikmitchell/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:784: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 2791}, page_content='The rabbit-hole went straight on like a tunnel for some way, and then\\ndipped suddenly down, so suddenly that Alice had not a moment to think\\nabout stopping herself before she found herself falling down a very\\ndeep well.'), -199.96148217980638), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 70057}, page_content='Alice waited a little, half expecting to see it again, but it did not\\nappear, and after a minute or two she walked on in the direction in\\nwhich the March Hare was said to live. “I’ve seen hatters before,” she\\nsaid to herself; “the March Hare will be much the most interesting, and'), -203.52694641732802), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 73039}, page_content='“Come, we shall have some fun now!” thought Alice. “I’m glad they’ve\\nbegun asking riddles.—I believe I can guess that,” she added aloud.\\n\\n“Do you mean that you think you can find out the answer to it?” said\\nthe March Hare.\\n\\n“Exactly so,” said Alice.'), -210.00581661578707), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 78508}, page_content='“Exactly so,” said the Hatter: “as the things get used up.”\\n\\n“But what happens when you come to the beginning again?” Alice ventured\\nto ask.\\n\\n“Suppose we change the subject,” the March Hare interrupted, yawning.\\n“I’m getting tired of this. I vote the young lady tells us a story.”'), -211.0177941429637), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 2680}, page_content='In another moment down went Alice after it, never once considering how\\nin the world she was to get out again.'), -218.1678849355636), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 75797}, page_content='“Have you guessed the riddle yet?” the Hatter said, turning to Alice\\nagain.\\n\\n“No, I give it up,” Alice replied: “what’s the answer?”\\n\\n“I haven’t the slightest idea,” said the Hatter.\\n\\n“Nor I,” said the March Hare.'), -220.01562435346762), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 93055}, page_content='Alice waited till the eyes appeared, and then nodded. “It’s no use\\nspeaking to it,” she thought, “till its ears have come, or at least one\\nof them.” In another minute the whole head appeared, and then Alice put\\ndown her flamingo, and began an account of the game, feeling very glad'), -221.6448960878776), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 79113}, page_content='“Tell us a story!” said the March Hare.\\n\\n“Yes, please do!” pleaded Alice.\\n\\n“And be quick about it,” added the Hatter, “or you’ll be asleep again\\nbefore it’s done.”'), -222.0255529366365)]\n",
      "  warnings.warn(\n",
      "/Users/nikmitchell/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:784: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 61888}, page_content='“Please would you tell me,” said Alice, a little timidly, for she was\\nnot quite sure whether it was good manners for her to speak first, “why\\nyour cat grins like that?”\\n\\n“It’s a Cheshire cat,” said the Duchess, “and that’s why. Pig!”'), -172.82051938818023), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 92811}, page_content='watching it a minute or two, she made it out to be a grin, and she said\\nto herself “It’s the Cheshire Cat: now I shall have somebody to talk\\nto.”'), -195.6172035245661), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 70519}, page_content='“Did you say pig, or fig?” said the Cat.\\n\\n“I said pig,” replied Alice; “and I wish you wouldn’t keep appearing\\nand vanishing so suddenly: you make one quite giddy.”'), -196.72594212372886), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 62320}, page_content='“I didn’t know that Cheshire cats always grinned; in fact, I didn’t\\nknow that cats could grin.”\\n\\n“They all can,” said the Duchess; “and most of ’em do.”\\n\\n“I don’t know of any that do,” Alice said very politely, feeling quite\\npleased to have got into a conversation.'), -204.22921946075257), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 62058}, page_content='“It’s a Cheshire cat,” said the Duchess, “and that’s why. Pig!”\\n\\nShe said the last word with such sudden violence that Alice quite\\njumped; but she saw in another moment that it was addressed to the\\nbaby, and not to her, so she took courage, and went on again:—'), -205.75724165239745), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 94250}, page_content='The Queen smiled and passed on.\\n\\n“Who are you talking to?” said the King, going up to Alice, and\\nlooking at the Cat’s head with great curiosity.\\n\\n“It’s a friend of mine—a Cheshire Cat,” said Alice: “allow me to\\nintroduce it.”'), -207.36344523650567), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 67869}, page_content='“Cheshire Puss,” she began, rather timidly, as she did not at all know\\nwhether it would like the name: however, it only grinned a little\\nwider. “Come, it’s pleased so far,” thought Alice, and she went on.\\n“Would you tell me, please, which way I ought to go from here?”'), -215.450396408293), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 94396}, page_content='“It’s a friend of mine—a Cheshire Cat,” said Alice: “allow me to\\nintroduce it.”\\n\\n“I don’t like the look of it at all,” said the King: “however, it may\\nkiss my hand if it likes.”\\n\\n“I’d rather not,” the Cat remarked.'), -215.96812424930334)]\n",
      "  warnings.warn(\n",
      "/Users/nikmitchell/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:784: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 78057}, page_content='“How dreadfully savage!” exclaimed Alice.\\n\\n“And ever since that,” the Hatter went on in a mournful tone, “he won’t\\ndo a thing I ask! It’s always six o’clock now.”\\n\\nA bright idea came into Alice’s head. “Is that the reason so many\\ntea-things are put out here?” she asked.'), -183.69422827570276), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 78329}, page_content='“Yes, that’s it,” said the Hatter with a sigh: “it’s always tea-time,\\nand we’ve no time to wash the things between whiles.”\\n\\n“Then you keep moving round, I suppose?” said Alice.\\n\\n“Exactly so,” said the Hatter: “as the things get used up.”'), -195.86273150785286), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 80253}, page_content='“Who’s making personal remarks now?” the Hatter asked triumphantly.\\n\\nAlice did not quite know what to say to this: so she helped herself to\\nsome tea and bread-and-butter, and then turned to the Dormouse, and\\nrepeated her question. “Why did they live at the bottom of a well?”'), -200.87844654909836), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 130097}, page_content='“Come, that finished the guinea-pigs!” thought Alice. “Now we shall get\\non better.”\\n\\n“I’d rather finish my tea,” said the Hatter, with an anxious look at\\nthe Queen, who was reading the list of singers.'), -201.40297183383657), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 77170}, page_content='“Is that the way you manage?” Alice asked.\\n\\nThe Hatter shook his head mournfully. “Not I!” he replied. “We\\nquarrelled last March—just before he went mad, you know—” (pointing\\nwith his tea spoon at the March Hare,) “—it was at the great concert\\ngiven by the Queen of Hearts, and I had to sing'), -203.17024246551486), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 68561}, page_content='“In that direction,” the Cat said, waving its right paw round, “lives\\na Hatter: and in that direction,” waving the other paw, “lives a\\nMarch Hare. Visit either you like: they’re both mad.”\\n\\n“But I don’t want to go among mad people,” Alice remarked.'), -203.8732707804647), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 75797}, page_content='“Have you guessed the riddle yet?” the Hatter said, turning to Alice\\nagain.\\n\\n“No, I give it up,” Alice replied: “what’s the answer?”\\n\\n“I haven’t the slightest idea,” said the Hatter.\\n\\n“Nor I,” said the March Hare.'), -204.52002057718536), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 77463}, page_content='‘Twinkle, twinkle, little bat!\\nHow I wonder what you’re at!’\\n\\nYou know the song, perhaps?”\\n\\n“I’ve heard something like it,” said Alice.\\n\\n“It goes on, you know,” the Hatter continued, “in this way:—\\n\\n‘Up above the world you fly,\\nLike a tea-tray in the sky.\\nTwinkle, twinkle—’”'), -205.09618485506869)]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How does Alice end up in Wonderland?\n",
      "Response: Alice ends up in Wonderland by following a rabbit down a rabbit-hole that suddenly dips down into a deep well.\n",
      "\n",
      "\n",
      "Question: What does the Cheshire cat tell Alice?\n",
      "Response: The Cheshire cat does not give Alice a specific answer but instead engages her in conversation, appearing and disappearing suddenly and making cryptic remarks.\n",
      "\n",
      "\n",
      "Question: What does Alice do with the Mad Hatter?\n",
      "Response: Alice has tea and bread-and-butter with the Mad Hatter and engages in conversation with him and the other characters at the tea party.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_description = \"alice_in_wonderland\" \n",
    "embeddings_description = \"ollama_embeddings\"\n",
    "answer_query_from_database(data_description, embeddings_description, show_source_passages=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">NOTE THE WARNINGS ABOUT HOW RELEVANCE SCORES SHOULD BE BETWEEN 0 AND 1</span>. It seems like the answers are sometimes better and sometimes worse than the OpenAI ones though? So maybe whatever calculation it is doing is actually preserving order?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

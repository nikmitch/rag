{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting hands-on experience with LLMs\n",
    "\n",
    "It seems like it will plausibly be valuable to be able to run LLMs locally on my laptop, or being able to hook into them for parts of tasks.\n",
    "\n",
    "Here's the rough kind of idea of what I want to learn in this project\n",
    "\n",
    "- Learn how to download, install and interact with a LLM (Llama3) hosted locally on my computer, sending it text directly and asking it simple questions\n",
    "- Learn how to interact with ChatGPT via an API, so I can do more automation and use it in coding projects\n",
    "- Understand environments better (through the course of debugging all this stuff) -- added post hoc lol - I screwed up my base conda installation when I was trying to follow one of the videos at the start of this process\n",
    "- Understand embeddings better, and build skills in visualisation to illustrate the distance between different words\n",
    "- Make a basic RAG that can read a larger document and answer basic questions from the text (from a file in .md or .txt format) - following [this video](https://www.youtube.com/watch?v=tcqEUSNCn8I). \n",
    " - Tried out both openai embeddings and ollama embeddings \n",
    "    - <span style=\"color:red\">Got negative similarity scores when doing a retrieval search from the database with ollama embeddings, which shouldn't be a thing (Should be between zero and 1)!. I found this [git issue](https://github.com/langchain-ai/langchain/issues/10864) logged on the langchain github with people mentioning this issue with a bunch of models (including 1 or 2 talking about getting the issue with locall llama in the last week). It does not yet appear to be resolved. Going to stick with OpenAI embeddings for now, and maybe check back in a while and see if anyone has a solution.</span>\n",
    "    - [This video](https://www.youtube.com/watch?v=2TJxpyO3ei4) might also help for the ollama version once the negative distance issue is resolved\n",
    "- extend to be able to read pdfs or arbitrary filetypes, using [this video](https://www.youtube.com/watch?v=2TJxpyO3ei4) then maybe [this video](https://www.youtube.com/watch?v=svzd5d1LXGk) -- or maybe another one entirely. [This documentation](https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/pdf/) might also be helpful\n",
    "\n",
    "[this link](https://github.com/langchain-ai/langchain/issues/14872) might help if I get Chroma readonly issues again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with ChatGPT via API\n",
    "\n",
    "This feels like it will be useful in a bunch of different projects. I've done some exploration of this (chat completion, embedding, image generation and text-to-speech) in `openai-test.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with Llama3 (no embedding)\n",
    "This next part is just me trying to interact with the model directly and feeding it a text file (no embedding etc), to see whether it'll provide sensible responses. \n",
    "I have already downloaded llama3 (the 4GB version - the 40GB version is way way too slow, basically doesn't run). Now I want to see if I can interact with it with the llama package.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "# note -- extremely bizarre that \"import ollama\" failed\n",
    "# after a successful-looking \"conda install ollama\" and required \n",
    "# me to \"pip install ollama\" in order to work??\n",
    "import os # will need this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I noticed that llama tends to print really long lines so I need to scroll sideways. I'm not enjoying that, so I'm making\n",
    "# a wrapped print function to fix it\n",
    "\n",
    "import textwrap\n",
    "\n",
    "def wprint(text, width = 120):\n",
    "  wrapped_text = textwrap.fill(text, width=width)\n",
    "  print(wrapped_text)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've downloaded a transcript of a YouTube video essay about how sound design is used in the recent Batman movie (see `data/personal/batman_sound_video_essay.txt`), and am prompting the model to answer basic questions about it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining function to take a question input and answer it with information from a specified file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_read_and_respond(input_file, question, print_prompt_with_data = False):\n",
    "    with open(input_file,'r') as file:\n",
    "        data = file.read()\n",
    "\n",
    "\n",
    "    #debugging statement to confirm file was loaded\n",
    "    if data:\n",
    "        print(\"File loaded successfully\")\n",
    "    else:\n",
    "        print(\"Load in a file\")\n",
    "\n",
    "\n",
    "    prompt_01 = f\"{data} #### From this text, {question}\"\n",
    "\n",
    "    if print_prompt_with_data:\n",
    "        wprint(\"Prompt: \"+prompt_01)\n",
    "\n",
    "    print(\"Generating a response: \")\n",
    "\n",
    "\n",
    "\n",
    "    response = ollama.chat(model = 'llama3',\n",
    "                            messages = [{\n",
    "                            \"role\":\"user\",\n",
    "                            #    \"content\":\"tell me about a cool species of frog\"\n",
    "                            \"content\": prompt_01\n",
    "                        }])\n",
    "\n",
    "    wprint(response[\"message\"][\"content\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions from Batman video essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully\n",
      "Generating a response: \n",
      "According to the text, the sound of rain is used in the movie in a way that creates ambiguity and tension. The author\n",
      "mentions that at first, we can clearly hear the rain, but then when gunfire breaks out, the rain suddenly falls away\n",
      "into silence. The author provides an example of this by creating a modified version of the scene where the sound of the\n",
      "rain remains audible even after the gunshots start, making it seem like the gunshots are quieter than they actually are.\n",
      "This manipulation of the sound of rain is used to create a sense of realism and to immerse the audience in the story.\n",
      "The author suggests that this technique is not about creating an accurate representation of reality but rather about\n",
      "creating an impressionistic and expressionist sound that feels right for the scene. By using this technique, the\n",
      "filmmakers are able to manipulate our perception of the sounds in the scene, adding to the tension and ambiguity of the\n",
      "moment.\n"
     ]
    }
   ],
   "source": [
    "llama_read_and_respond(input_file='data/personal/batman_sound_video_essay.txt', \n",
    "                       question = 'tell me about how the sound of rain is used in the movie')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully\n",
      "Generating a response: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the text, the sponsor of the video is Nebula, a streaming platform. The video also mentions a \"Curiosity\n",
      "Stream\" bundle offer, which seems to be related to Nebula as well.\n"
     ]
    }
   ],
   "source": [
    "llama_read_and_respond(input_file='data/personal/batman_sound_video_essay.txt', \n",
    "                       question = 'who sponsored the video?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this seems to be working. Now I'd like it to try reading something from my CV, because it seemed to be struggling with that when I was running it from the terminal. I've just changed the extension from a .tex file to .txt, and I want to see if it can answer basic questions (e.g. about dates of employment). This might be harder for it to do because it's still got all of these latex formatting things in there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions from CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully\n",
      "Generating a response: \n",
      "According to the text, the most recent job listed is:  **NZ Royal Commission Inquiry - COVID-19 Lessons Learned**\n",
      "**Principal Data Analyst (May 2024 -- July 2024)**  In this role, you created high-quality visualizations to support the\n",
      "Inquiry, including:  1. Visualizations that contextualized pandemic trends (COVID-19 cases, hospitalizations, deaths,\n",
      "and vaccinations) in New Zealand against policy decisions (e.g., lockdowns, border closures) and pandemic trends in\n",
      "other countries. 2. Analyses and visualizations highlighting the disparate impact of COVID-19 on Māori and Pacific\n",
      "ethnic groups and people living in areas of higher socioeconomic deprivation. 3. Worked closely with the Chair of the\n",
      "Commission to discuss how to tell the story of the COVID pandemic through these visualizations, drawing out lessons for\n",
      "future pandemics.\n"
     ]
    }
   ],
   "source": [
    "llama_read_and_respond(input_file='data/personal/Nik_Mitchell_CV_2024_07_21.txt', \n",
    "                       question = 'what is the most recent job on that list, and what did I do in that job?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the correct answer but it seems to really directly copy-paste exactly what I wrote in my bullet points here. Next, asking it to be more concise & summarise a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully\n",
      "Generating a response: \n",
      "The most recent job listed is \"Principal Data Analyst\" at the NZ Royal Commission Inquiry - COVID-19 Lessons Learned,\n",
      "which took place from May 2024 to July 2024.  In this role, I was responsible for:  * Creating high-quality\n",
      "visualizations to support the inquiry * Conducting analyses and creating visualizations to highlight pandemic trends and\n",
      "disparities in Māori and Pacific ethnic groups and people living in areas of higher socioeconomic deprivation * Working\n",
      "closely with the Chair of the Commission to discuss how to tell the story of the COVID-19 pandemic through\n",
      "visualizations  Please note that this summary is based on the provided LaTeX code, which may not accurately reflect my\n",
      "actual responsibilities or experiences.\n"
     ]
    }
   ],
   "source": [
    "llama_read_and_respond(input_file='data/personal/Nik_Mitchell_CV_2024_07_21.txt', \n",
    "                       question = 'what is the most recent job on that list, and what did I do in that job? Please be concise and summarise the responsibilities rather than copying the whole description')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has shaved off a few words without changing the meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (Retrieval-Augmented Generator)\n",
    "\n",
    "Why would we want to create a RAG? The above seemed to work just fine.\n",
    "\n",
    "I have a suspicion that the issue here is to do with context windows. When making a RAG, we're first going to create a database by chunking up all the inputs into manageable-sized pieces (with overlap between chunks) and then using particular embeddings to encode the meaning of the chunks as vectors. Once we have that, we can use the same embeddings on the input question, and then retrieve the top few chunks that have the most similar meaning vectors (e.g. smallest euclidean distance apart) and use this subset of data to construct the answer from.\n",
    "\n",
    "I suspect that the reason for creating a RAG is this is a context window limitation. The LLM needs to know which information to focus on, so having a method for retrieving the most relevant data allows it to work much more efficiently with a large amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now working through [this video](https://www.youtube.com/watch?v=tcqEUSNCn8I)(about how to make a RAG) - will use OpenAI embeddings here rather than Llama.\n",
    "\n",
    "Has an associated [git repo](https://github.com/pixegami/langchain-rag-tutorial) - might clone this.\n",
    "\n",
    "I've grabbed a version of the Wizard of Oz from the Gutenberg Project website [link](https://www.gutenberg.org/ebooks/55)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning investigation\n",
    "\n",
    "I'm kinda curious to try to build something a bit more flexible here, and use that to investigate a few questions\n",
    "- Does it make a difference if you use OpenAIEmbeddings() or OllamaEmbeddings()?\n",
    "- Can I build several different chromadbs with different embeddings for different datasets\n",
    "    - Wizard of Oz\n",
    "    - Alice in Wonderland\n",
    "    - My personal files (CV, batman video essay)\n",
    "        - does it matter if I mash these together into a single database, even though they're about totally different things?\n",
    "- do you get better performance with bigger chunks?\n",
    "- can I extend this to read PDF files?\n",
    "\n",
    "I'm a bit worried about doing this if it's not on the mainline to being able to do AI safety work, but I also think that just being curious and following my nose and making functions to output different things and label files and folders appropriately in python etc is going to be valuable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import openai \n",
    "# from dotenv import load_dotenv\n",
    "import os\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chroma_path(data_description, embeddings_description):\n",
    "    CHROMA_PATH=os.path.join(\"chroma\",data_description, embeddings_description)\n",
    "    return CHROMA_PATH\n",
    "\n",
    "def get_data_path(data_description):\n",
    "    DATA_PATH =os.path.join(\"data\",data_description)\n",
    "    return DATA_PATH\n",
    "\n",
    "def get_embedding_function(embeddings_description):\n",
    "    if embeddings_description == \"openai_embeddings\":\n",
    "        embedding_function = OpenAIEmbeddings()\n",
    "    elif embeddings_description == \"ollama_embeddings\":\n",
    "        embedding_function = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    else:\n",
    "        print(\"please specify either 'openai_embeddings' or 'ollama_embeddings'\")\n",
    "    return embedding_function\n",
    "\n",
    "\n",
    "\n",
    "def generate_data_store(data_description, embeddings_description):\n",
    "\n",
    "    CHROMA_PATH= get_chroma_path(data_description, embeddings_description)\n",
    "    DATA_PATH =  get_data_path(data_description)\n",
    "\n",
    "    print(f\"Data source: {data_description}, Embeddings: {embeddings_description}\")\n",
    "\n",
    "    # print(f\"CHROMA_PATH is {CHROMA_PATH}\")\n",
    "    # print(f\"DATA_PATH is {DATA_PATH}\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    documents = load_documents(data_path=DATA_PATH)\n",
    "    chunks = split_text(documents)\n",
    "    save_to_chroma(chunks, get_embedding_function(embeddings_description), chroma_path= CHROMA_PATH)\n",
    "\n",
    "\n",
    "def load_documents(data_path):\n",
    "    loader = DirectoryLoader(data_path, glob=\"*.md\")\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "\n",
    "def split_text(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "    ## print test example\n",
    "    # document = chunks[10]\n",
    "    # print(document.page_content)\n",
    "    # print(document.metadata)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def save_to_chroma(chunks: list[Document], embedding_function, chroma_path):\n",
    "    # Clear out the database first.\n",
    "    if os.path.exists(chroma_path):\n",
    "        shutil.rmtree(chroma_path)\n",
    "\n",
    "    # Create a new DB from the documents.\n",
    "    db = Chroma.from_documents(\n",
    "        chunks, embedding_function, persist_directory=chroma_path\n",
    "    )\n",
    "    db.persist()\n",
    "    print(f\"Saved {len(chunks)} chunks to {chroma_path}.\")\n",
    "\n",
    "\n",
    "# generate_data_store(data_description       = data_descriptions[0],\n",
    "#                     embeddings_description = embeddings_descriptions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source: wizard_of_oz, Embeddings: openai_embeddings\n",
      "Split 1 documents into 1127 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikmitchell/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1127 chunks to chroma/wizard_of_oz/openai_embeddings.\n",
      "Data source: alice_in_wonderland, Embeddings: openai_embeddings\n",
      "Split 1 documents into 801 chunks.\n",
      "Saved 801 chunks to chroma/alice_in_wonderland/openai_embeddings.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "# data_descriptions = [\"wizard_of_oz\",\"alice_in_wonderland\",\"personal\"] ## Commenting out personal for now because it does't use markdown files\n",
    "data_descriptions = [\"wizard_of_oz\",\"alice_in_wonderland\"]\n",
    "# embeddings_descriptions = [\"openai_embeddings\",\"ollama_embeddings\"]\n",
    "embeddings_descriptions = [\"openai_embeddings\"] # Commenting out ollama for now because it's not working\n",
    "\n",
    "for data_description, embeddings_description in itertools.product(data_descriptions, embeddings_descriptions):\n",
    "    generate_data_store(data_description, embeddings_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have already downloaded llama3 (the 4GB version - the 40GB version is way way too slow, basically doesn't run). Now I want to see if I can interact with it with the llama package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, that works. This is exciting. I should get the question-asking part running up soon too.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding investigation\n",
    "\n",
    "I'm also curious now about the embeddings, and how they work for ollama versus openai. So I looked into it in `embeddings_investigation.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering questions\n",
    "\n",
    "Initially we just have the code to do the openai embeddings and chat to openAI. Is it possible to use the OpenAI embeddings and generate the response with Llama3? My guess is yes, but also that the quality of the answers will depend primarily on the quality of the embeddings, since the AI model can't answer correctly if the correct information isn't retrieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to ask several basic question questions to pull info from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import argparse\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Define a dictionary to hold lists of questions for each book\n",
    "question_dict = {\n",
    "    \"wizard_of_oz\": [\n",
    "        \"How does Dorothy get back home?\",\n",
    "        \"What is the role of the Tin Man?\",\n",
    "        \"What obstacles do Dorothy and her friends face on their journey?\"\n",
    "    ],\n",
    "    \"alice_in_wonderland\": [\n",
    "        \"How does Alice end up in Wonderland?\",\n",
    "        \"What characters does Alice meet along her journey?\",\n",
    "        \"What are the key events in Alice's adventure?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def get_query_list(data_description):\n",
    "    \"\"\"\n",
    "    Return a list of questions for the specified book description.\n",
    "    \"\"\"\n",
    "    return question_dict.get(data_description, [])\n",
    "\n",
    "def answer_query_from_database(data_description, embeddings_description, show_source_passages=False):\n",
    "    \"\"\"\n",
    "    Answer queries from the database based on the book description and embeddings description.\n",
    "    Optionally show source passages if show_source_passages is True.\n",
    "    \"\"\"\n",
    "    chroma_path = get_chroma_path(data_description, embeddings_description)\n",
    "    embedding_function = get_embedding_function(embeddings_description)\n",
    "\n",
    "    db = Chroma(\n",
    "        persist_directory=chroma_path,\n",
    "        embedding_function=embedding_function\n",
    "    )\n",
    "    \n",
    "    print(f\"Loading the Chroma database from {chroma_path}, using the {embeddings_description} embedding function.\")\n",
    "\n",
    "    # Get the list of questions for the specified book\n",
    "    query_list = get_query_list(data_description)\n",
    "    \n",
    "    # Initialize an empty string to store responses\n",
    "    all_responses = \"\"\n",
    "\n",
    "    for query_text in query_list:\n",
    "        # Search the DB\n",
    "        results = db.similarity_search_with_relevance_scores(query_text, k=8)\n",
    "        \n",
    "        if len(results) == 0:  # Removed score check\n",
    "        # if len(results) == 0 or results[0][1] < 0.7:\n",
    "            print(f\"Unable to find matching results for the query: {query_text}\")\n",
    "            continue\n",
    "\n",
    "        # Prepare the context text\n",
    "        context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "        prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "        prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "\n",
    "        # print(prompt)\n",
    "\n",
    "        # Get response from the model\n",
    "        model = ChatOpenAI()\n",
    "        response_text = model.predict(prompt)\n",
    "\n",
    "        # Retrieve sources\n",
    "        if show_source_passages:\n",
    "            sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "            formatted_response = f\"Question: {query_text}\\nResponse: {response_text}\\nSources: {sources}\"\n",
    "        else:\n",
    "            formatted_response = f\"Question: {query_text}\\nResponse: {response_text}\"\n",
    "\n",
    "        # Append the response to the all_responses string\n",
    "        all_responses += formatted_response + \"\\n\\n\\n\"\n",
    "\n",
    "    # Print the final responses\n",
    "    print(all_responses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wizard of Oz Qs\n",
    "\n",
    "#### OpenAI embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_chroma_path(data_description, embeddings_description):\n",
    "#     CHROMA_PATH=os.path.join(\"chroma\",data_description, embeddings_description)\n",
    "#     return CHROMA_PATH\n",
    "\n",
    "# def get_embedding_function(embeddings_description):\n",
    "#     if embeddings_description == \"openai_embeddings\":\n",
    "#         embedding_function = OpenAIEmbeddings()\n",
    "#     elif embeddings_description == \"ollama_embeddings\":\n",
    "#         embedding_function = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "#     else:\n",
    "#         print(\"please specify either 'openai_embeddings' or 'ollama_embeddings'\")\n",
    "#     return embedding_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the Chroma database from chroma/wizard_of_oz/openai_embeddings, using the openai_embeddings embedding function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikmitchell/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How does Dorothy get back home?\n",
      "Response: Dorothy gets back home by clicking the heels of her shoes together three times and saying, \"Take me home to Aunt Em!\" This magic transports her back to Kansas, but her Silver Shoes fall off during the journey and are lost forever.\n",
      "\n",
      "\n",
      "Question: What is the role of the Tin Man?\n",
      "Response: The role of the Tin Woodman is to be rescued by his friends and to eventually be sent for by Oz.\n",
      "\n",
      "\n",
      "Question: What obstacles do Dorothy and her friends face on their journey?\n",
      "Response: Dorothy and her friends face obstacles such as a long and sometimes dangerous journey through pleasant and dark terrain, trees that seem to be fighting them and trying to stop their journey, rough and difficult roads with uneven and broken yellow bricks, and the challenge of getting to the Witch's castle.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_description = \"wizard_of_oz\" \n",
    "embeddings_description = \"openai_embeddings\"\n",
    "answer_query_from_database(data_description, embeddings_description, show_source_passages=False)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ollama embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the Chroma database from chroma/wizard_of_oz/ollama_embeddings, using the ollama_embeddings embedding function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikmitchell/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:784: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 207034}, page_content='Dorothy now took Toto up solemnly in her arms, and having said one last\\ngood-bye she clapped the heels of her shoes together three times,\\nsaying:\\n\\n“Take me home to Aunt Em!”\\n\\nInstantly she was whirling through the air, so swiftly that all she\\ncould see or feel was the wind whistling past her ears.'), -210.30281095872743), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 72930}, page_content='“We must hurry and get back to the road of yellow brick before dark,”\\nhe said; and the Scarecrow agreed with him. So they kept walking until\\nDorothy could stand no longer. Her eyes closed in spite of herself and\\nshe forgot where she was and fell among the poppies, fast asleep.'), -211.1797890955506), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 178514}, page_content='“You are now our ruler,” he said to the Scarecrow; “so you must come\\nback to us as soon as possible.”\\n\\n“I certainly shall if I am able,” the Scarecrow replied; “but I must\\nhelp Dorothy to get home, first.”\\n\\nAs Dorothy bade the good-natured Guardian a last farewell she said:'), -213.68763582658013), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 25454}, page_content='This worried Dorothy a little, but she knew that only the Great Oz\\ncould help her get to Kansas again, so she bravely resolved not to turn\\nback.'), -214.08434759004857), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 87806}, page_content='“And I want him to send me back to Kansas,” said Dorothy.\\n\\n“Where is Kansas?” asked the man, with surprise.\\n\\n“I don’t know,” replied Dorothy sorrowfully, “but it is my home, and\\nI’m sure it’s somewhere.”'), -214.84481969929524), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 102961}, page_content='Sorrowfully Dorothy left the Throne Room and went back where the Lion\\nand the Scarecrow and the Tin Woodman were waiting to hear what Oz had\\nsaid to her. “There is no hope for me,” she said sadly, “for Oz will\\nnot send me home until I have killed the Wicked Witch of the West; and'), -215.5650897842087), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 204361}, page_content='Then the Witch looked at the big, shaggy Lion and asked, “When Dorothy\\nhas returned to her own home, what will become of you?”'), -216.55052491206715), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 134867}, page_content='Now that they were reunited, Dorothy and her friends spent a few happy\\ndays at the Yellow Castle, where they found everything they needed to\\nmake them comfortable.\\n\\nBut one day the girl thought of Aunt Em, and said, “We must go back to\\nOz, and claim his promise.”'), -220.5110177365151)]\n",
      "  warnings.warn(\n",
      "/Users/nikmitchell/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:784: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 159923}, page_content='“That must be a matter of opinion,” said the Tin Woodman. “For my part,\\nI will bear all the unhappiness without a murmur, if you will give me\\nthe heart.”'), -265.4799490913427), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 43252}, page_content='“And why is that?” asked the Scarecrow.\\n\\n“I will tell you my story, and then you will know.”\\n\\nSo, while they were walking through the forest, the Tin Woodman told\\nthe following story:'), -273.1379276159374), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 74635}, page_content='“We can do nothing for him,” said the Tin Woodman, sadly; “for he is\\nmuch too heavy to lift. We must leave him here to sleep on forever, and\\nperhaps he will dream that he has found courage at last.”'), -274.8077250624771), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 47167}, page_content='“I shall take the heart,” returned the Tin Woodman; “for brains do not\\nmake one happy, and happiness is the best thing in the world.”'), -291.19075130181835), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 41235}, page_content='“Why do you wish to see Oz?” he asked.\\n\\n“I want him to send me back to Kansas, and the Scarecrow wants him to\\nput a few brains into his head,” she replied.\\n\\nThe Tin Woodman appeared to think deeply for a moment. Then he said:\\n\\n“Do you suppose Oz could give me a heart?”'), -293.08704388920694), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 106736}, page_content='“I am Oz, the Great and Terrible,” spoke the Beast, in a voice that was\\none great roar. “Who are you, and why do you seek me?”\\n\\n“I am a Woodman, and made of tin. Therefore I have no heart, and cannot\\nlove. I pray you to give me a heart that I may be as other men are.”'), -298.1730639198661), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 42728}, page_content='“Why didn’t you walk around the hole?” asked the Tin Woodman.\\n\\n“I don’t know enough,” replied the Scarecrow cheerfully. “My head is\\nstuffed with straw, you know, and that is why I am going to Oz to ask\\nhim for some brains.”'), -299.8797617752141), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 159714}, page_content='“How about my heart?” asked the Tin Woodman.\\n\\n“Why, as for that,” answered Oz, “I think you are wrong to want a\\nheart. It makes most people unhappy. If you only knew it, you are in\\nluck not to have a heart.”'), -300.00477000224697)]\n",
      "  warnings.warn(\n",
      "/Users/nikmitchell/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:784: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 18275}, page_content='“How can I get there?” asked Dorothy.\\n\\n“You must walk. It is a long journey, through a country that is\\nsometimes pleasant and sometimes dark and terrible. However, I will use\\nall the magic arts I know of to keep you from harm.”'), -194.15342257094588), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 16415}, page_content='“I am anxious to get back to my aunt and uncle, for I am sure they will\\nworry about me. Can you help me find my way?”\\n\\nThe Munchkins and the Witch first looked at one another, and then at\\nDorothy, and then shook their heads.'), -205.66406272535997), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 25454}, page_content='This worried Dorothy a little, but she knew that only the Great Oz\\ncould help her get to Kansas again, so she bravely resolved not to turn\\nback.'), -207.00635285933663), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 175627}, page_content='“This little girl,” said the Scarecrow to the soldier, “wishes to cross\\nthe desert. How can she do so?”\\n\\n“I cannot tell,” answered the soldier, “for nobody has ever crossed the\\ndesert, unless it is Oz himself.”\\n\\n“Is there no one who can help me?” asked Dorothy earnestly.'), -210.9502512894141), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 891}, page_content='Introduction\\n Chapter I. The Cyclone\\n Chapter II. The Council with the Munchkins\\n Chapter III. How Dorothy Saved the Scarecrow\\n Chapter IV. The Road Through the Forest\\n Chapter V. The Rescue of the Tin Woodman\\n Chapter VI.  The Cowardly Lion\\n Chapter VII. The Journey to the Great Oz'), -215.43229147087186), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 100074}, page_content='“I am Oz, the Great and Terrible. Who are you, and why do you seek me?”\\n\\nIt was not such an awful voice as she had expected to come from the big\\nHead; so she took courage and answered:\\n\\n“I am Dorothy, the Small and Meek. I have come to you for help.”'), -218.78794127866095), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 109665}, page_content='“What shall we do now?” asked Dorothy sadly.\\n\\n“There is only one thing we can do,” returned the Lion, “and that is to\\ngo to the land of the Winkies, seek out the Wicked Witch, and destroy\\nher.”\\n\\n“But suppose we cannot?” said the girl.\\n\\n“Then I shall never have courage,” declared the Lion.'), -220.49140225604333), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 202089}, page_content='“What can I do for you, my child?” she asked.\\n\\nDorothy told the Witch all her story: how the cyclone had brought her\\nto the Land of Oz, how she had found her companions, and of the\\nwonderful adventures they had met with.'), -221.35726711185114)]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How does Dorothy get back home?\n",
      "Response: Dorothy gets back home by asking the Great Oz to send her back to Kansas.\n",
      "\n",
      "\n",
      "Question: What is the role of the Tin Man?\n",
      "Response: The role of the Tin Woodman is to seek a heart from the Great and Terrible Oz so that he can experience love and happiness like other men.\n",
      "\n",
      "\n",
      "Question: What obstacles do Dorothy and her friends face on their journey?\n",
      "Response: Dorothy and her friends face obstacles such as a long and sometimes dangerous journey, the uncertainty of finding their way back home, the challenge of crossing the desert, the intimidating presence of the Great Oz, the need to seek out and destroy the Wicked Witch, and the struggle to find courage in the face of adversity.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_description = \"wizard_of_oz\" \n",
    "embeddings_description = \"ollama_embeddings\"\n",
    "answer_query_from_database(data_description, embeddings_description, show_source_passages=False)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">NOTE THE WARNINGS ABOUT HOW RELEVANCE SCORES SHOULD BE BETWEEN 0 AND 1</span>. It seems like the answers aren't that much worse than the OpenAI ones though? So maybe whatever calculation it is doing is actually preserving order?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alice in wonderland Qs\n",
    "\n",
    "#### OpenAI embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the Chroma database from chroma/alice_in_wonderland/openai_embeddings, using the openai_embeddings embedding function.\n",
      "Question: How does Alice end up in Wonderland?\n",
      "Response: Alice ends up in Wonderland by following a White Rabbit down a rabbit-hole, which leads her to fall into a deep well.\n",
      "\n",
      "\n",
      "Question: What characters does Alice meet along her journey?\n",
      "Response: Alice meets the White Rabbit, the Mouse, the March Hare, the Queen, the Knave of Hearts, the Hatter, and the footmen with powdered hair.\n",
      "\n",
      "\n",
      "Question: What are the key events in Alice's adventure?\n",
      "Response: 1. Alice trying to find her way out of the hall with locked doors.\n",
      "2. Alice deciding to grow to her right size and find her way into the garden.\n",
      "3. Alice being asked to settle a question by three characters.\n",
      "4. Alice telling the two creatures about her adventures with the White Rabbit.\n",
      "5. Alice finding a tiny golden key on a glass table.\n",
      "6. Alice forgetting the key when trying to enter the garden.\n",
      "7. Alice explaining her adventures to the Mock Turtle during her evidence.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_description = \"alice_in_wonderland\" \n",
    "embeddings_description = \"openai_embeddings\"\n",
    "answer_query_from_database(data_description, embeddings_description, show_source_passages=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ollama embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the Chroma database from chroma/alice_in_wonderland/ollama_embeddings, using the ollama_embeddings embedding function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikmitchell/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:784: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 70057}, page_content='Alice waited a little, half expecting to see it again, but it did not\\nappear, and after a minute or two she walked on in the direction in\\nwhich the March Hare was said to live. “I’ve seen hatters before,” she\\nsaid to herself; “the March Hare will be much the most interesting, and'), -218.61949414932923), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 78508}, page_content='“Exactly so,” said the Hatter: “as the things get used up.”\\n\\n“But what happens when you come to the beginning again?” Alice ventured\\nto ask.\\n\\n“Suppose we change the subject,” the March Hare interrupted, yawning.\\n“I’m getting tired of this. I vote the young lady tells us a story.”'), -219.66867419392426), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 75797}, page_content='“Have you guessed the riddle yet?” the Hatter said, turning to Alice\\nagain.\\n\\n“No, I give it up,” Alice replied: “what’s the answer?”\\n\\n“I haven’t the slightest idea,” said the Hatter.\\n\\n“Nor I,” said the March Hare.'), -228.82584511708325), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 73039}, page_content='“Come, we shall have some fun now!” thought Alice. “I’m glad they’ve\\nbegun asking riddles.—I believe I can guess that,” she added aloud.\\n\\n“Do you mean that you think you can find out the answer to it?” said\\nthe March Hare.\\n\\n“Exactly so,” said Alice.'), -229.63769726923778), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 78057}, page_content='“How dreadfully savage!” exclaimed Alice.\\n\\n“And ever since that,” the Hatter went on in a mournful tone, “he won’t\\ndo a thing I ask! It’s always six o’clock now.”\\n\\nA bright idea came into Alice’s head. “Is that the reason so many\\ntea-things are put out here?” she asked.'), -240.88295658884255), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 57352}, page_content='is that to be done, I wonder?” As she said this, she came suddenly\\nupon an open place, with a little house in it about four feet high.\\n“Whoever lives there,” thought Alice, “it’ll never do to come upon them\\nthis size: why, I should frighten them out of their wits!” So she'), -241.54420759884925), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 67869}, page_content='“Cheshire Puss,” she began, rather timidly, as she did not at all know\\nwhether it would like the name: however, it only grinned a little\\nwider. “Come, it’s pleased so far,” thought Alice, and she went on.\\n“Would you tell me, please, which way I ought to go from here?”'), -244.02587877673), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 95834}, page_content='the other side of the garden, where Alice could see it trying in a\\nhelpless sort of way to fly up into a tree.'), -245.12479884606364)]\n",
      "  warnings.warn(\n",
      "/Users/nikmitchell/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:784: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 70057}, page_content='Alice waited a little, half expecting to see it again, but it did not\\nappear, and after a minute or two she walked on in the direction in\\nwhich the March Hare was said to live. “I’ve seen hatters before,” she\\nsaid to herself; “the March Hare will be much the most interesting, and'), -230.48813300674271), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 68561}, page_content='“In that direction,” the Cat said, waving its right paw round, “lives\\na Hatter: and in that direction,” waving the other paw, “lives a\\nMarch Hare. Visit either you like: they’re both mad.”\\n\\n“But I don’t want to go among mad people,” Alice remarked.'), -235.98616444405454), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 99016}, page_content='“Perhaps it hasn’t one,” Alice ventured to remark.\\n\\n“Tut, tut, child!” said the Duchess. “Everything’s got a moral, if only\\nyou can find it.” And she squeezed herself up closer to Alice’s side as\\nshe spoke.'), -241.1918842218829), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 18734}, page_content='railway,” she said to herself. (Alice had been to the seaside once in\\nher life, and had come to the general conclusion, that wherever you go\\nto on the English coast you find a number of bathing machines in the\\nsea, some children digging in the sand with wooden spades, then a row'), -245.16942460361656), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 31646}, page_content='“I wish I hadn’t mentioned Dinah!” she said to herself in a melancholy\\ntone. “Nobody seems to like her, down here, and I’m sure she’s the best\\ncat in the world! Oh, my dear Dinah! I wonder if I shall ever see you\\nany more!” And here poor Alice began to cry again, for she felt very'), -247.720516024267), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 47361}, page_content='Which brought them back again to the beginning of the conversation.\\nAlice felt a little irritated at the Caterpillar’s making such very\\nshort remarks, and she drew herself up and said, very gravely, “I\\nthink, you ought to tell me who you are, first.”\\n\\n“Why?” said the Caterpillar.'), -248.13216058475024), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 78057}, page_content='“How dreadfully savage!” exclaimed Alice.\\n\\n“And ever since that,” the Hatter went on in a mournful tone, “he won’t\\ndo a thing I ask! It’s always six o’clock now.”\\n\\nA bright idea came into Alice’s head. “Is that the reason so many\\ntea-things are put out here?” she asked.'), -249.34907564552455), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 73039}, page_content='“Come, we shall have some fun now!” thought Alice. “I’m glad they’ve\\nbegun asking riddles.—I believe I can guess that,” she added aloud.\\n\\n“Do you mean that you think you can find out the answer to it?” said\\nthe March Hare.\\n\\n“Exactly so,” said Alice.'), -249.6338130105684)]\n",
      "  warnings.warn(\n",
      "/Users/nikmitchell/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:784: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 78057}, page_content='“How dreadfully savage!” exclaimed Alice.\\n\\n“And ever since that,” the Hatter went on in a mournful tone, “he won’t\\ndo a thing I ask! It’s always six o’clock now.”\\n\\nA bright idea came into Alice’s head. “Is that the reason so many\\ntea-things are put out here?” she asked.'), -232.54909636583648), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 90635}, page_content='“Get to your places!” shouted the Queen in a voice of thunder, and\\npeople began running about in all directions, tumbling up against each\\nother; however, they got settled down in a minute or two, and the game\\nbegan. Alice thought she had never seen such a curious croquet-ground'), -242.01210909837525), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 79113}, page_content='“Tell us a story!” said the March Hare.\\n\\n“Yes, please do!” pleaded Alice.\\n\\n“And be quick about it,” added the Hatter, “or you’ll be asleep again\\nbefore it’s done.”'), -245.49863667190823), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 78508}, page_content='“Exactly so,” said the Hatter: “as the things get used up.”\\n\\n“But what happens when you come to the beginning again?” Alice ventured\\nto ask.\\n\\n“Suppose we change the subject,” the March Hare interrupted, yawning.\\n“I’m getting tired of this. I vote the young lady tells us a story.”'), -249.22559954072872), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 109902}, page_content='“What a curious plan!” exclaimed Alice.\\n\\n“That’s the reason they’re called lessons,” the Gryphon remarked:\\n“because they lessen from day to day.”\\n\\nThis was quite a new idea to Alice, and she thought it over a little\\nbefore she made her next remark. “Then the eleventh day must have been\\na holiday?”'), -250.3721448745301), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 1087}, page_content='CHAPTER VI. Pig and Pepper\\nCHAPTER VII. A Mad Tea-Party\\nCHAPTER VIII. The Queen’s Croquet-Ground\\nCHAPTER IX. The Mock Turtle’s Story\\nCHAPTER X. The Lobster Quadrille\\nCHAPTER XI. Who Stole the Tarts?\\nCHAPTER XII. Alice’s Evidence'), -250.75737651081374), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 27664}, page_content='“But who is to give the prizes?” quite a chorus of voices asked.\\n\\n“Why, she, of course,” said the Dodo, pointing to Alice with one\\nfinger; and the whole party at once crowded round her, calling out in a\\nconfused way, “Prizes! Prizes!”'), -256.60227179092317), (Document(metadata={'source': 'data/alice_in_wonderland/alice_in_wonderland.md', 'start_index': 130097}, page_content='“Come, that finished the guinea-pigs!” thought Alice. “Now we shall get\\non better.”\\n\\n“I’d rather finish my tea,” said the Hatter, with an anxious look at\\nthe Queen, who was reading the list of singers.'), -256.8229405514339)]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How does Alice end up in Wonderland?\n",
      "Response: Alice ends up in Wonderland by following the direction in which the March Hare was said to live.\n",
      "\n",
      "\n",
      "Question: What characters does Alice meet along her journey?\n",
      "Response: Alice meets the March Hare, the Hatter, the Duchess, the Caterpillar, and the Cheshire Cat along her journey.\n",
      "\n",
      "\n",
      "Question: What are the key events in Alice's adventure?\n",
      "Response: Some key events in Alice's adventure include attending a mad tea-party with the Hatter, March Hare, and other characters, playing a curious game of croquet with the Queen, listening to the Mock Turtle's story, questioning the concept of lessons that lessen each day, being asked to tell a story by the March Hare and Hatter, and being chosen to give out prizes by the Dodo.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_description = \"alice_in_wonderland\" \n",
    "embeddings_description = \"ollama_embeddings\"\n",
    "answer_query_from_database(data_description, embeddings_description, show_source_passages=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">NOTE THE WARNINGS ABOUT HOW RELEVANCE SCORES SHOULD BE BETWEEN 0 AND 1</span>. It seems like the answers aren't that much worse than the OpenAI ones though? So maybe whatever calculation it is doing is actually preserving order?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting hands-on experience with LLMs\n",
    "\n",
    "It seems like it will plausibly be valuable to be able to run LLMs locally on my laptop, or being able to hook into them for parts of tasks.\n",
    "\n",
    "Here's the rough kind of idea of what I want to learn in this notebook\n",
    "\n",
    "- Learn how to download, install and interact with a LLM (Llama3) hosted locally on my computer, sending it text directly and asking it simple questions\n",
    "- Learn how to interact with ChatGPT via an API, so I can do more automation and use it in coding projects\n",
    "- Understand environments better (through the course of debugging all this stuff) -- added post hoc lol - I screwed up my base conda installation when I was trying to follow one of the videos at the start of this process\n",
    "- Understand embeddings better, and build skills in visualisation to illustrate the distance between different words\n",
    "- Make a basic RAG that can read a larger document and answer basic questions from the text (from a file in .md or .txt format) - following [this video](https://www.youtube.com/watch?v=tcqEUSNCn8I). Try out different embeddings (Ollama embeddings and OpenAI embeddings)\n",
    "    - [This video](https://www.youtube.com/watch?v=2TJxpyO3ei4) might help for the local version \n",
    "- extend to be able to read pdfs or arbitrary filetypes, using [this video](https://www.youtube.com/watch?v=2TJxpyO3ei4) then maybe [this video](https://www.youtube.com/watch?v=svzd5d1LXGk) -- or maybe another one entirely. [This documentation](https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/pdf/) might also be helpful\n",
    "\n",
    "[this link](https://github.com/langchain-ai/langchain/issues/14872) might help if I get Chroma readonly issues again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "I have already downloaded llama3 (the 4GB version - the 40GB version is way way too slow, basically doesn't run). Now I want to see if I can interact with it with the llama package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "# note -- extremely bizarre that \"import ollama\" failed\n",
    "# after a successful-looking \"conda install ollama\" and required \n",
    "# me to \"pip install ollama\" in order to work??\n",
    "import os # will need this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I noticed that llama tends to print really long lines so I need to scroll sideways. I'm not enjoying that, so I'm making\n",
    "# a wrapped print function to fix it\n",
    "\n",
    "import textwrap\n",
    "\n",
    "def wprint(text, width = 120):\n",
    "  wrapped_text = textwrap.fill(text, width=width)\n",
    "  print(wrapped_text)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with Llama3 (no embedding)\n",
    "This next part is just me trying to interact with the model directly and feeding it a text file (no embedding etc), to see whether it'll provide sensible responses. \n",
    "\n",
    "I've downloaded a transcript of a YouTube video essay about how sound design is used in the recent Batman movie (see `data/personal/batman_sound_video_essay.txt`), and am prompting the model to answer basic questions about it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_read_and_respond(input_file, question, print_prompt_with_data = False):\n",
    "    with open(input_file,'r') as file:\n",
    "        data = file.read()\n",
    "\n",
    "\n",
    "    #debugging statement to confirm file was loaded\n",
    "    if data:\n",
    "        print(\"File loaded successfully\")\n",
    "    else:\n",
    "        print(\"Load in a file\")\n",
    "\n",
    "\n",
    "    prompt_01 = f\"{data} #### From this text, {question}\"\n",
    "\n",
    "    if print_prompt_with_data:\n",
    "        wprint(\"Prompt: \"+prompt_01)\n",
    "\n",
    "    print(\"Generating a response: \")\n",
    "\n",
    "\n",
    "\n",
    "    response = ollama.chat(model = 'llama3',\n",
    "                            messages = [{\n",
    "                            \"role\":\"user\",\n",
    "                            #    \"content\":\"tell me about a cool species of frog\"\n",
    "                            \"content\": prompt_01\n",
    "                        }])\n",
    "\n",
    "    wprint(response[\"message\"][\"content\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully\n",
      "Generating a response: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the video, in one scene where gunfire breaks out, the sound of rain suddenly falls away into silence. The\n",
      "narrator then provides a modified version of the scene where the sound of rain remains audible, and the gunshots\n",
      "actually sound quieter. This demonstrates how the filmmakers used the sound design to create an impressionistic and\n",
      "expressionist effect, rather than aiming for realism. The goal is not to accurately represent the sounds of reality but\n",
      "to create a certain atmosphere or mood that enhances the storytelling.\n"
     ]
    }
   ],
   "source": [
    "llama_read_and_respond(input_file='data/personal/batman_sound_video_essay.txt', \n",
    "                       question = 'tell me about how the sound of rain is used in the movie')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully\n",
      "Generating a response: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the text, Nebula, a streaming platform created by and for independent content creators like the speaker, is\n",
      "sponsoring the video.\n"
     ]
    }
   ],
   "source": [
    "llama_read_and_respond(input_file='data/personal/batman_sound_video_essay.txt', \n",
    "                       question = 'who sponsored the video?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this seems to be working. Now I'd like it to try reading something from my CV, because it seemed to be struggling with that when I was running it from the terminal. I've just changed the extension from a .tex file to .txt, and I want to see if it can answer basic questions (e.g. about dates of employment). This might be harder for it to do because it's still got all of these latex formatting things in there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully\n",
      "Generating a response: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most recent job listed is:  **NZ Royal Commission Inquiry - COVID-19 Lessons Learned**  As a **Principal Data\n",
      "Analyst**, from May 2024 to July 2024, you:  * Created high-quality visualisations to support the Inquiry * Conducted\n",
      "analyses and created visualisations to highlight the disparate impact of COVID-19 on Māori and Pacific ethnic groups and\n",
      "people living in areas of higher socioeconomic deprivation * Worked closely with the Chair of the Commission to discuss\n",
      "how to tell the story of the COVID pandemic through the above visualisations in a way that draws out lessons for future\n",
      "pandemics.\n"
     ]
    }
   ],
   "source": [
    "llama_read_and_respond(input_file='data/personal/Nik_Mitchell_CV_2024_07_21.txt', \n",
    "                       question = 'what is the most recent job on that list, and what did I do in that job?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the correct answer but it seems to really directly copy-paste exactly what I wrote in my bullet points here. Next, asking it to be more concise & summarise a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully\n",
      "Generating a response: \n",
      "The most recent job listed is \"Principal Data Analyst\" at NZ Royal Commission Inquiry - COVID-19 Lessons Learned (May\n",
      "2024 - July 2024). As a Principal Data Analyst, my responsibilities included:  * Creating high-quality visualizations to\n",
      "support the inquiry * Conducting analyses and creating visualizations to highlight pandemic trends and disparities in\n",
      "impact on Māori and Pacific ethnic groups and people living in areas of higher socioeconomic deprivation * Working\n",
      "closely with the Chair of the Commission to discuss how to tell the story of the COVID-19 pandemic through\n",
      "visualizations\n"
     ]
    }
   ],
   "source": [
    "llama_read_and_respond(input_file='data/personal/Nik_Mitchell_CV_2024_07_21.txt', \n",
    "                       question = 'what is the most recent job on that list, and what did I do in that job? Please be concise and summarise the responsibilities rather than copying the whole description')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has shaved off a few words without changing the meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with ChatGPT via API\n",
    "\n",
    "This feels like it will be useful in a bunch of different projects. I've done some exploration of this (chat completion, embedding, image generation and text-to-speech) in `openai-test.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (Retrieval-Augmented Generator)\n",
    "\n",
    "Why would we want to create a RAG? The above seemed to work just fine.\n",
    "\n",
    "I have a suspicion that the issue here is to do with context windows. When making a RAG, we're first going to create a database by chunking up all the inputs into manageable-sized pieces (with overlap between chunks) and then using particular embeddings to encode the meaning of the chunks as vectors. Once we have that, we can use the same embeddings on the input question, and then retrieve the top few chunks that have the most similar meaning vectors (e.g. smallest euclidean distance apart) and use this subset of data to construct the answer from.\n",
    "\n",
    "I suspect that the reason for creating a RAG is this is a context window limitation. The LLM needs to know which information to focus on, so having a method for retrieving the most relevant data allows it to work much more efficiently with a large amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now working through [this video](https://www.youtube.com/watch?v=tcqEUSNCn8I)(about how to make a RAG) - will use OpenAI embeddings here rather than Llama.\n",
    "\n",
    "Has an associated [git repo](https://github.com/pixegami/langchain-rag-tutorial) - might clone this.\n",
    "\n",
    "I've grabbed a version of the Wizard of Oz from the Gutenberg Project website [link](https://www.gutenberg.org/ebooks/55)\n",
    "\n",
    "### Creating the database\n",
    "\n",
    "First loading in a bunch of packages I'll need\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import openai \n",
    "# from dotenv import load_dotenv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = \"data/wizard_of_oz\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next going to define some functions to use to load the data, split it into chunks, and then turn it into a Chroma database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_documents():\n",
    "    loader = DirectoryLoader(DATA_PATH, glob=\"*.md\")\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "\n",
    "def split_text(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "    document = chunks[10]\n",
    "    print(document.page_content)\n",
    "    print(document.metadata)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def save_to_chroma(chunks: list[Document]):\n",
    "    # Clear out the database first.\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "    # Create a new DB from the documents.\n",
    "    db = Chroma.from_documents(\n",
    "        chunks, OpenAIEmbeddings(), persist_directory=CHROMA_PATH\n",
    "    )\n",
    "    db.persist()\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the functions to create the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_documents()\n",
    "chunks = split_text(documents)\n",
    "save_to_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 documents into 1127 chunks.\n",
      "Introduction\n",
      "{'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 1870}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error raised by inference API HTTP code: 500, {\"error\":\"error loading model /Users/nikmitchell/.ollama/models/blobs/sha256:970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfe\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 62\u001b[0m\n\u001b[1;32m     58\u001b[0m     db\u001b[38;5;241m.\u001b[39mpersist()\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m chunks to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHROMA_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 62\u001b[0m \u001b[43mgenerate_data_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 23\u001b[0m, in \u001b[0;36mgenerate_data_store\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m documents \u001b[38;5;241m=\u001b[39m load_documents()\n\u001b[1;32m     22\u001b[0m chunks \u001b[38;5;241m=\u001b[39m split_text(documents)\n\u001b[0;32m---> 23\u001b[0m \u001b[43msave_to_chroma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 55\u001b[0m, in \u001b[0;36msave_to_chroma\u001b[0;34m(chunks)\u001b[0m\n\u001b[1;32m     52\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mrmtree(CHROMA_PATH)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Create a new DB from the documents.\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m db \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOllamaEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnomic-embed-text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCHROMA_PATH\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m db\u001b[38;5;241m.\u001b[39mpersist()\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m chunks to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHROMA_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_community/vectorstores/chroma.py:790\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    789\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 790\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_community/vectorstores/chroma.py:748\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[1;32m    743\u001b[0m         api\u001b[38;5;241m=\u001b[39mchroma_collection\u001b[38;5;241m.\u001b[39m_client,\n\u001b[1;32m    744\u001b[0m         ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[1;32m    745\u001b[0m         metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[1;32m    746\u001b[0m         documents\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[1;32m    747\u001b[0m     ):\n\u001b[0;32m--> 748\u001b[0m         \u001b[43mchroma_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m            \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    754\u001b[0m     chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(texts\u001b[38;5;241m=\u001b[39mtexts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, ids\u001b[38;5;241m=\u001b[39mids)\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_community/vectorstores/chroma.py:276\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 276\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[1;32m    280\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_community/embeddings/ollama.py:211\u001b[0m, in \u001b[0;36mOllamaEmbeddings.embed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Embed documents using an Ollama deployed embedding model.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    210\u001b[0m instruction_pairs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_instruction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[0;32m--> 211\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstruction_pairs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_community/embeddings/ollama.py:199\u001b[0m, in \u001b[0;36mOllamaEmbeddings._embed\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_emb_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m iter_]\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_community/embeddings/ollama.py:173\u001b[0m, in \u001b[0;36mOllamaEmbeddings._process_emb_response\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError raised by inference endpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError raised by inference API HTTP code: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;241m%\u001b[39m (res\u001b[38;5;241m.\u001b[39mstatus_code, res\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m     t \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[0;31mValueError\u001b[0m: Error raised by inference API HTTP code: 500, {\"error\":\"error loading model /Users/nikmitchell/.ollama/models/blobs/sha256:970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfe\"}"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import openai\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Load environment variables. Assumes that project contains .env file with API keys\n",
    "# Set OpenAI API key\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = \"data/wizard_of_oz\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_data_store():\n",
    "    documents = load_documents()\n",
    "    chunks = split_text(documents)\n",
    "    save_to_chroma(chunks)\n",
    "\n",
    "\n",
    "def load_documents():\n",
    "    loader = DirectoryLoader(DATA_PATH, glob=\"*.md\")\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "\n",
    "def split_text(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "    document = chunks[10]\n",
    "    print(document.page_content)\n",
    "    print(document.metadata)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def save_to_chroma(chunks: list[Document]):\n",
    "    # Clear out the database first.\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "    # Create a new DB from the documents.\n",
    "    db = Chroma.from_documents(\n",
    "        chunks, OllamaEmbeddings(model=\"nomic-embed-text\"), persist_directory=CHROMA_PATH\n",
    "    )\n",
    "    db.persist()\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n",
    "\n",
    "\n",
    "generate_data_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikmitchell/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:784: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 37936}, page_content='“It must be inconvenient to be made of flesh,” said the Scarecrow\\nthoughtfully, “for you must sleep, and eat and drink. However, you have\\nbrains, and it is worth a lot of bother to be able to think properly.”'), -248.2220163170763), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 104484}, page_content='“Why should I do this for you?” asked the Lady.\\n\\n“Because you are wise and powerful, and no one else can help me,”\\nanswered the Scarecrow.'), -249.9570692234033), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 78361}, page_content='“Nothing that I know of,” answered the Woodman; but the Scarecrow, who\\nhad been trying to think, but could not because his head was stuffed\\nwith straw, said, quickly, “Oh, yes; you can save our friend, the\\nCowardly Lion, who is asleep in the poppy bed.”'), -257.10161416508873)]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 37936}, page_content='“It must be inconvenient to be made of flesh,” said the Scarecrow\\nthoughtfully, “for you must sleep, and eat and drink. However, you have\\nbrains, and it is worth a lot of bother to be able to think properly.”'),\n",
       "  -248.2220163170763),\n",
       " (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 104484}, page_content='“Why should I do this for you?” asked the Lady.\\n\\n“Because you are wise and powerful, and no one else can help me,”\\nanswered the Scarecrow.'),\n",
       "  -249.9570692234033),\n",
       " (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 78361}, page_content='“Nothing that I know of,” answered the Woodman; but the Scarecrow, who\\nhad been trying to think, but could not because his head was stuffed\\nwith straw, said, quickly, “Oh, yes; you can save our friend, the\\nCowardly Lion, who is asleep in the poppy bed.”'),\n",
       "  -257.10161416508873)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = Chroma(persist_directory=CHROMA_PATH, embedding_function=OllamaEmbeddings(model=\"nomic-embed-text\"))\n",
    "db.similarity_search_with_relevance_scores(\"scarecrow needs\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning investigation\n",
    "\n",
    "I'm kinda curious to try to build something a bit more flexible here, and use that to investigate a few questions\n",
    "- Does it make a difference if you use OpenAIEmbeddings() or OllamaEmbeddings()?\n",
    "- Can I build several different chromadbs with different embeddings for different datasets\n",
    "    - Wizard of Oz\n",
    "    - Alice in Wonderland\n",
    "    - My personal files (CV, batman video essay)\n",
    "        - does it matter if I mash these together into a single database, even though they're about totally different things?\n",
    "- do you get better performance with bigger chunks?\n",
    "- can I extend this to read PDF files?\n",
    "\n",
    "I'm a bit worried about doing this if it's not on the mainline to being able to do AI safety work, but I also think that just being curious and following my nose and making functions to output different things and label files and folders appropriately in python etc is going to be valuable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chroma_path(data_description, embeddings_description):\n",
    "    CHROMA_PATH=os.path.join(\"chroma\",data_description, embeddings_description)\n",
    "    return CHROMA_PATH\n",
    "\n",
    "def get_data_path(data_description):\n",
    "    DATA_PATH =os.path.join(\"data\",data_description)\n",
    "    return DATA_PATH\n",
    "\n",
    "def get_embedding_function(embeddings_description):\n",
    "    if embeddings_description == \"openai_embeddings\":\n",
    "        embedding_function = OpenAIEmbeddings()\n",
    "    elif embeddings_description == \"ollama_embeddings\":\n",
    "        embedding_function = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    else:\n",
    "        print(\"please specify either 'openai_embeddings' or 'ollama_embeddings'\")\n",
    "    return embedding_function\n",
    "\n",
    "\n",
    "\n",
    "def generate_data_store(data_description, embeddings_description):\n",
    "\n",
    "    CHROMA_PATH= get_chroma_path(data_description, embeddings_description)\n",
    "    DATA_PATH =  get_data_path(data_description)\n",
    "\n",
    "    print(f\"Data source: {data_description}, Embeddings: {embeddings_description}\")\n",
    "\n",
    "    # print(f\"CHROMA_PATH is {CHROMA_PATH}\")\n",
    "    # print(f\"DATA_PATH is {DATA_PATH}\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    documents = load_documents(data_path=DATA_PATH)\n",
    "    chunks = split_text(documents)\n",
    "    save_to_chroma(chunks, get_embedding_function(embeddings_description), chroma_path= CHROMA_PATH)\n",
    "\n",
    "\n",
    "def load_documents(data_path):\n",
    "    loader = DirectoryLoader(data_path, glob=\"*.md\")\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "\n",
    "def split_text(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "    ## print test example\n",
    "    # document = chunks[10]\n",
    "    # print(document.page_content)\n",
    "    # print(document.metadata)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def save_to_chroma(chunks: list[Document], embedding_function, chroma_path):\n",
    "    # Clear out the database first.\n",
    "    if os.path.exists(chroma_path):\n",
    "        shutil.rmtree(chroma_path)\n",
    "\n",
    "    # Create a new DB from the documents.\n",
    "    db = Chroma.from_documents(\n",
    "        chunks, embedding_function, persist_directory=chroma_path\n",
    "    )\n",
    "    db.persist()\n",
    "    print(f\"Saved {len(chunks)} chunks to {chroma_path}.\")\n",
    "\n",
    "\n",
    "# generate_data_store(data_description       = data_descriptions[0],\n",
    "#                     embeddings_description = embeddings_descriptions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source: wizard_of_oz, Embeddings: openai_embeddings\n",
      "Split 1 documents into 1127 chunks.\n",
      "Saved 1127 chunks to chroma/wizard_of_oz/openai_embeddings.\n",
      "Data source: wizard_of_oz, Embeddings: ollama_embeddings\n",
      "Split 1 documents into 1127 chunks.\n",
      "Saved 1127 chunks to chroma/wizard_of_oz/ollama_embeddings.\n",
      "Data source: alice_in_wonderland, Embeddings: openai_embeddings\n",
      "Split 1 documents into 801 chunks.\n",
      "Saved 801 chunks to chroma/alice_in_wonderland/openai_embeddings.\n",
      "Data source: alice_in_wonderland, Embeddings: ollama_embeddings\n",
      "Split 1 documents into 801 chunks.\n",
      "Saved 801 chunks to chroma/alice_in_wonderland/ollama_embeddings.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "# data_descriptions = [\"wizard_of_oz\",\"alice_in_wonderland\",\"personal\"] ## Commenting out personal for now because it does't use markdown files\n",
    "data_descriptions = [\"wizard_of_oz\",\"alice_in_wonderland\"]\n",
    "embeddings_descriptions = [\"openai_embeddings\",\"ollama_embeddings\"]\n",
    "\n",
    "for data_description, embeddings_description in itertools.product(data_descriptions, embeddings_descriptions):\n",
    "    generate_data_store(data_description, embeddings_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, that works. This is exciting. I should get the question-asking part running up soon too.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answering questions\n",
    "\n",
    "Initially we just have the code to do the openai embeddings and chat to openAI. Is it possible to use the OpenAI embeddings and generate the response with Llama3? My guess is yes, but also that the quality of the answers will depend primarily on the quality of the embeddings, since the AI model can't answer correctly if the correct information isn't retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## commented version of query_data.py\n",
    "\n",
    "# import argparse\n",
    "# # from dataclasses import dataclass\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# CHROMA_PATH = \"chroma\"\n",
    "\n",
    "# PROMPT_TEMPLATE = \"\"\"\n",
    "# Answer the question based only on the following context:\n",
    "\n",
    "# {context}\n",
    "\n",
    "# ---\n",
    "\n",
    "# Answer the question based on the above context: {question}\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     # Create CLI.\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\"query_text\", type=str, help=\"The query text.\")\n",
    "#     args = parser.parse_args()\n",
    "#     query_text = args.query_text\n",
    "\n",
    "#     # Prepare the DB.\n",
    "#     embedding_function = OpenAIEmbeddings()\n",
    "#     db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\n",
    "#     # Search the DB.\n",
    "#     results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "#     if len(results) == 0 or results[0][1] < 0.7:\n",
    "#         print(f\"Unable to find matching results.\")\n",
    "#         return\n",
    "\n",
    "#     context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "#     prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "#     prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "#     print(prompt)\n",
    "\n",
    "#     model = ChatOpenAI()\n",
    "#     response_text = model.predict(prompt)\n",
    "\n",
    "#     sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "#     formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "#     print(formatted_response)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "# from dataclasses import dataclass\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_list(data_description):\n",
    "    if data_description==\"wizard_of_oz\":\n",
    "        query_text = \"How does Dorothy get back home?\"\n",
    "    elif data_description==\"alice_in_wonderland\":\n",
    "        query_text = \"How does Alice end up in Wonderland?\"\n",
    "    return query_text\n",
    "\n",
    "def answer_query_from_database(data_description, embeddings_description, query_text):\n",
    "    # don't need a CLI any more\n",
    "    # parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument(\"query_text\", type=str, help=\"The query text.\")\n",
    "    # args = parser.parse_args()\n",
    "    # query_text = args.query_text\n",
    "\n",
    "    # Prepare the DB.\n",
    "    # embedding_function = OpenAIEmbeddings()\n",
    "    chroma_path = get_chroma_path(data_description, embeddings_description)\n",
    "    embedding_function=get_embedding_function(embeddings_description)\n",
    "\n",
    "    db = Chroma(persist_directory=chroma_path, \n",
    "                embedding_function=embedding_function)\n",
    "    \n",
    "    print(f\"loading in the chroma database from {chroma_path},  using the {embedding_function} embedding function\")\n",
    "\n",
    "    # Search the DB.\n",
    "    results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "    if len(results) == 0 or results[0][1] < 0.7:\n",
    "        print(f\"Unable to find matching results.\")\n",
    "        return\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "    print(prompt)\n",
    "\n",
    "    model = ChatOpenAI()\n",
    "    response_text = model.predict(prompt)\n",
    "\n",
    "    sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing openai embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading in the chroma database from chroma/wizard_of_oz/openai_embeddings,  using the client=<openai.resources.embeddings.Embeddings object at 0x13c242870> async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x1485fd340> model='text-embedding-ada-002' dimensions=None deployment='text-embedding-ada-002' openai_api_version='' openai_api_base=None openai_api_type='' openai_proxy='' embedding_ctx_length=8191 openai_api_key=SecretStr('**********') openai_organization=None allowed_special=None disallowed_special=None chunk_size=1000 max_retries=2 request_timeout=None headers=None tiktoken_enabled=True tiktoken_model_name=None show_progress_bar=False model_kwargs={} skip_empty=False default_headers=None default_query=None retry_min_seconds=4 retry_max_seconds=20 http_client=None http_async_client=None check_embedding_ctx_length=True embedding function\n",
      "Human: \n",
      "Answer the question based only on the following context:\n",
      "\n",
      "“How can I get to her castle?” asked Dorothy.\n",
      "\n",
      "---\n",
      "\n",
      "“How can I get there?” asked Dorothy.\n",
      "\n",
      "“You must walk. It is a long journey, through a country that is\n",
      "sometimes pleasant and sometimes dark and terrible. However, I will use\n",
      "all the magic arts I know of to keep you from harm.”\n",
      "\n",
      "---\n",
      "\n",
      "This worried Dorothy a little, but she knew that only the Great Oz\n",
      "could help her get to Kansas again, so she bravely resolved not to turn\n",
      "back.\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: How does Dorothy get back home?\n",
      "\n",
      "Response: Dorothy resolves to walk through a long journey, with the help of the magic arts of the person she is speaking to, in order to get back home to Kansas.\n",
      "Sources: ['data/wizard_of_oz/wizard_of_oz.md', 'data/wizard_of_oz/wizard_of_oz.md', 'data/wizard_of_oz/wizard_of_oz.md']\n"
     ]
    }
   ],
   "source": [
    "answer_query_from_database(data_description=\"wizard_of_oz\", \n",
    "                           embeddings_description=\"openai_embeddings\",\n",
    "                           query_text = get_query_list(\"wizard_of_oz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'answer_query_from_database' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43manswer_query_from_database\u001b[49m(data_description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malice_in_wonderland\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      2\u001b[0m                            embeddings_description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m                            query_text \u001b[38;5;241m=\u001b[39m get_query_list(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malice_in_wonderland\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'answer_query_from_database' is not defined"
     ]
    }
   ],
   "source": [
    "answer_query_from_database(data_description=\"alice_in_wonderland\", \n",
    "                           embeddings_description=\"openai_embeddings\",\n",
    "                           query_text = get_query_list(\"alice_in_wonderland\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading in the chroma database from chroma/wizard_of_oz/ollama_embeddings,  using the base_url='http://localhost:11434' model='nomic-embed-text' embed_instruction='passage: ' query_instruction='query: ' mirostat=None mirostat_eta=None mirostat_tau=None num_ctx=None num_gpu=None num_thread=None repeat_last_n=None repeat_penalty=None temperature=None stop=None tfs_z=None top_k=None top_p=None show_progress=False headers=None model_kwargs=None embedding function\n",
      "Unable to find matching results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikmitchell/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:784: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 207034}, page_content='Dorothy now took Toto up solemnly in her arms, and having said one last\\ngood-bye she clapped the heels of her shoes together three times,\\nsaying:\\n\\n“Take me home to Aunt Em!”\\n\\nInstantly she was whirling through the air, so swiftly that all she\\ncould see or feel was the wind whistling past her ears.'), -210.30281095872743), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 72930}, page_content='“We must hurry and get back to the road of yellow brick before dark,”\\nhe said; and the Scarecrow agreed with him. So they kept walking until\\nDorothy could stand no longer. Her eyes closed in spite of herself and\\nshe forgot where she was and fell among the poppies, fast asleep.'), -211.1797890955506), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 207867}, page_content='Dorothy stood up and found she was in her stocking-feet. For the Silver\\nShoes had fallen off in her flight through the air, and were lost\\nforever in the desert.\\n\\nChapter XXIV\\nHome Again'), -211.1823138603638)]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "answer_query_from_database(data_description=\"wizard_of_oz\", \n",
    "                           embeddings_description=\"ollama_embeddings\", \n",
    "                           query_text = get_query_list(\"wizard_of_oz\")\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <span style=\"color: red; font-weight: bold;\">The ollama embeddings don't seem to be working at all - getting these negative relevance scores, which makes me feel like I haven't done the original embeddings for the database properly (or I'm not embedding the query properly)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding investigation\n",
    "\n",
    "I'm also curious now about the embeddings, and how they work for ollama versus openai. So I looked into it in `embeddings_investigation.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

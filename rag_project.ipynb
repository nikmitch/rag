{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting hands-on experience with LLMs\n",
    "\n",
    "It seems like it will plausibly be valuable to be able to run LLMs locally on my laptop, or being able to hook into them for parts of tasks.\n",
    "\n",
    "This might be a basic thing that I could do that I could put on a blog or something\n",
    "\n",
    "\n",
    "Here's the rough kind of idea I had based on watching various youtube videos, will need to assess whether it's going to be actually useful\n",
    "\n",
    "- Download latest Llama model and see if I can interact with it using Ollama through an interface in the terminal\n",
    "- try to use it (or maybe the OpenAI embeddings) to make a basic RAG that can read a document and answer basic questions from the text (from a file in .txt format) - following [this video](https://www.youtube.com/watch?v=tcqEUSNCn8I)\n",
    "- extend to be able to read pdfs or arbitrary filetypes, using [this video](https://www.youtube.com/watch?v=2TJxpyO3ei4) then maybe [this video](https://www.youtube.com/watch?v=svzd5d1LXGk)\n",
    "\n",
    "\n",
    "I initially tried to follow [this video](https://www.youtube.com/watch?v=ztBJqzBU5kc), but I found myself running into a lot of issues with package installation. I screwed up my base conda installation, needed to reset it and make a new environment (which I titled 'llama') for this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "I have already downloaded llama3 (the 4GB version - the 40GB version is way way too slow, basically doesn't run). Now I want to see if I can interact with it with the llama package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "# note -- extremely bizarre that \"import ollama\" failed\n",
    "# after a successful-looking \"conda install ollama\" and required \n",
    "# me to \"pip install ollama\" in order to work??\n",
    "import os # will need this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I noticed that llama tends to print really long lines so I need to scroll sideways. I'm not enjoying that, so I'm making\n",
    "# a wrapped print function to fix it\n",
    "\n",
    "import textwrap\n",
    "\n",
    "def wprint(text, width = 120):\n",
    "  wrapped_text = textwrap.fill(text, width=width)\n",
    "  print(wrapped_text)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with Llama3 (no embedding)\n",
    "This next part is just me trying to interact with the model directly and feeding it a text file (no embedding etc), to see whether it'll provide sensible responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('data/personal/batman_sound_video_essay.txt','r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "\n",
    "#debugging statement to confirm file was loaded\n",
    "if data:\n",
    "    print(\"File loaded successfully\")\n",
    "else:\n",
    "    print(\"Load in a file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<agent_01> Prompt: Why The Batman's Sound is Different - YouTube https://www.youtube.com/watch?v=_AQkQ4a1yJ8\n",
      "Transcript: (00:01) listen to this this otherworldly sound happens to be a justin bieber song slowed down 800 percent\n",
      "and it's this contorted version of the song that was responsible in part for inspiring what would eventually become the\n",
      "sound of the batmobile in the latest batman film [Music] this video is sponsored by curiositystream watch the exclusive\n",
      "companion video on nebula when you sign up for the curiosity stream nebula bundle today watch this clip on mute a truck\n",
      "passes in front of the frame and imagine with me for a moment what sound (00:48) you think will accompany that truck got\n",
      "it okay here it is i must choose my targets carefully instead of a truck we hear a train but there's no visual sign of a\n",
      "train in this shot or any of the shots in close proximity the film establishes trains as part of the setting but when\n",
      "the sound plays we don't see a train the sound of the train passing fades out as the truck passes it's almost as if the\n",
      "truck was making the sound i'm guessing if you've seen the movie you didn't even notice or (01:19) think about this but\n",
      "this small detail reveals something about sound in film and especially how it's used in the batman that i think is\n",
      "fascinating gotham city in the new batman film is soaked in rain i don't think i've seen a film that uses rain this\n",
      "relentlessly since david fincher's seven which this movie is obviously drawing some inspiration from it's impressive the\n",
      "range of effects and moods that the sound team for the batman manages to get out of the sound of the rain this rain\n",
      "hitting the bat signal at the beginning (01:48) almost has a sinister sizzling feel the rain in this scene feels cold\n",
      "icy and almost sleet like the rain in a more intimate setting like this one or this one feels warmer and cozier and in\n",
      "this scene where batman's doing some hard-boiled detective investigating the rain in the background almost sounds like\n",
      "boiling or simmering water if you listen closely the rain has a unique sound in almost every scene or location in the\n",
      "film and the differences in the sounds don't just reflect the fact that rain sounds different in (02:32) different\n",
      "environments they actually design the sound of the rain to reinforce the mood of each scene i've talked before about\n",
      "expressionism and cinematography and similar to how harsh shadows and lighting and even set design can become an\n",
      "external manifestation of a character's internal psychological state sound can express these things as well but the way\n",
      "something sounds isn't the only way you can use sound expressively the mix of the sound its volume and the relative\n",
      "volume of different sound elements in the scene (03:02) can be used in an expressionistic way take this moment [Music]\n",
      "if you were standing in a room and it was raining outside you could hear the rain and as long as it's still raining in\n",
      "the real world the volume of the rain outside isn't going to change but let's turn down the dialogue in this scene and i\n",
      "want you to listen closely to what's happening with the sound of the rain in the background [Music] [Music] here the\n",
      "rain which is a prominent part of the ambient soundscape softens and almost drops away completely for a (03:54) moment\n",
      "to emphasize the tension between batman and selena before coming back in towards the end the mix here is modeling the\n",
      "sound of the film around the subjective experience of the characters in the real world when you're focused in on\n",
      "something you tend not to pay attention to the ambient sounds around you they're still there at a constant volume but\n",
      "you're not actually hearing them right now in whatever environment you're watching this in there are probably all kinds\n",
      "of external sounds that you (04:25) haven't been listening to while you're listening to me but if i draw your attention\n",
      "to them like i'm doing now you probably suddenly become aware of them in our subjective experience of the world what we\n",
      "hear reflects what we're paying attention to so by manipulating the level of certain ambient sounds the sound team is\n",
      "placing you as the viewer inside the character's subjective perspective this kind of subjectivity isn't just created\n",
      "through levels listen to this moment and pay close attention to the (04:54) sound of the camera flashes he's wearing\n",
      "gloves friday july 16th my life has been a cruel riddle i could not solve hear how they become echoey and reverby as\n",
      "gordon reads the riddler's words in this scene we're inside batman's auditory perspective here the sound design isn't\n",
      "replicating a real world effect in how we perceive sound just like we don't actually see in slow motion but sometimes\n",
      "cinematographers use that to create a subjective effect there's a kind of dissociative quality to this kind of (05:29)\n",
      "echoey effect that helps us understand the headspace the characters are in in a film sound's relationship to reality can\n",
      "be very loose instead of crafting a soundscape that is realistic sound designers and engineers have a lot of leeway in a\n",
      "film to design sounds that create the right impression of reality or that reflect a character's subjective experience\n",
      "rather than what the environment or the objects would actually sound like [Music] i'm sure we're all familiar with the\n",
      "idea of score using a motif or theme to (05:59) cue a certain character this film obviously has one for batman that it\n",
      "employs in sometimes strikingly subtle ways like in this moment where it literally only plays the first note of the\n",
      "theme as the words the batman appear on screen we often think of a film soundtrack as coming from a kind of objective\n",
      "perspective where the film itself is telling us what the emotions of the scene are or what's happening but film score\n",
      "can be used subjectively as well in this moment when selena's theme plays as this boot is revealed the film's (06:30)\n",
      "score isn't telling us objectively that it's selena because it turns out not to be the film is telling us that bruce\n",
      "thinks it's selena my favorite soundtrack cue in the film is the riddler's ave maria which is used expertly for this\n",
      "scene the association between the riddler and the song is established earlier in the film and here the fact that the\n",
      "choir sings it before the riddler shows up gives the whole scene a sense of foreboding and suspense but by placing the\n",
      "music in the scene diegetically (07:03) which just means that the sound is actually coming from within the world of the\n",
      "film instead of just one that's part of the omniscient soundtrack it creates a sense of ambiguity that pervades the\n",
      "scene the ambiguity doesn't just come from the song we also hear this sound which sounds like the riddler's signature\n",
      "gasp scream thing but then it ends in a cough so we're not quite sure if it's supposed to actually be the riddler or not\n",
      "which is the entire tension of the scene bruce is paranoid and is seeing the riddler (07:35) everywhere and the film\n",
      "does a great job of helping us enter into that ambiguity and mystery through the sound design one of the interesting\n",
      "auditory tools that this movie uses kind of exists in the space between sound effect and soundtrack listen to this\n",
      "ambient soundscape it's kind of an underwater droning and it crops up pretty much any time riddler is on screen and\n",
      "batman is looking at or listening to him it's not a piece of film score but it's also not really a sound effect it's not\n",
      "motivated (08:12) by any element of the environment but this sound unlike ave maria also isn't a cue for the riddler\n",
      "it's more of a cue for batman's emotional response to the riddler here that underwater ambience comes in right after\n",
      "falcone is shot giving us an early indication that the batman thinks the riddler is responsible and then it shifts to\n",
      "the ave maria theme as that suspicion is confirmed after the riddler is unmasked and the batman has a confrontation with\n",
      "him the next time we see a video of the riddler (08:51) that ambient sound bed is absent hey guys thanks for all the\n",
      "comments and uh because the batman's perspective of the riddler has changed i'm honestly kind of in awe of the extensive\n",
      "work and artistry that goes into the sound design on a film like this it takes entire teams of people an incredible\n",
      "amount of work and a lot of thought goes into crafting every part of how a film like this sounds and a lot of the\n",
      "techniques used aren't even intuitive for example why is this scene so impactful the sound of the club just seems so\n",
      "(09:30) intense and overwhelming well it turns out to make the scene feel the way it does the sound team discovered that\n",
      "they actually needed to make the sound of the punches quieter compare the sound of these hits to these from moments\n",
      "earlier it's a mild difference but in the mix these hits are actually quieter than these here's a modified version of\n",
      "the scene where i brought up the levels of some of these punches it changes the whole feel of the scene when we're\n",
      "watching a film we understand the volume of sounds relative to each (10:05) other so if the punches in this scene are\n",
      "clearly audible the music feels way quieter if we were in the room and the music was really blaring you wouldn't be able\n",
      "to hear these hits another example of this kind of thing is in this scene where we can clearly hear the rain but then\n",
      "moments later once gunfire breaks out the rain falls away into silence listen to this modified version where the sound\n",
      "of the rain remains audible the gunshots actually sound quieter even though i haven't changed their level all (10:44)\n",
      "i've done is leave the rain at the volume it was at moments before the key takeaway here is that almost nothing you hear\n",
      "in a film like this is a reflection of reality the sound is almost entirely impressionistic and expressionist the idea\n",
      "is to create a sound that feels right to the audience not one that would be accurate if closely examined because realism\n",
      "isn't the goal you often can't achieve what you want simply by going out into the world and recording real world sounds\n",
      "the batmobile is a great example of this the sound design (11:28) combines the engine sounds from multiple sources but\n",
      "there's a key ingredient that comes from a curious place that's the sound of a bottle rocket going off supervising sound\n",
      "editor will files liked the sound and wanted to incorporate it into the sound design of the batmobile but it was too\n",
      "short and felt too quick and small it was ultimately the same technique that was used to create this strange ethereal\n",
      "version of this justin bieber song that morphed the bottle rocket sound into the sound of the batmobile (12:02) that's\n",
      "in the film by stretching the same bottle rocket sound out into different lengths and then layering those together viles\n",
      "and his team were able to build the unique rising scream that helps make the batmobile reveal so impactful [Music] and\n",
      "by modifying a real sound instead of trying to generate it synthetically they maintained a physicality and imperfect\n",
      "quality in the sound that's a better fit for the film and the batmobile's grungy dirty aesthetic subjective perspective\n",
      "plays an incredibly important role in the story (12:41) of the batman and i've been talking about how that subjectivity\n",
      "is created through sound but it's also developed in cinematography in a way i think is crucial to the story the movie is\n",
      "trying to tell you can see more about how the film develops and uses visual perspective in the companion video i've made\n",
      "exclusively available on nebula nebula is a streaming platform created by and for independent content creators like\n",
      "myself on nebula you can watch every video from me and a ton of other amazing creators early and without any (13:09) ads\n",
      "or sponsorships but you also get access to a bunch of exclusive nebula content like my video on perspective in batman or\n",
      "my video on the sound design in dune which goes into some of the cool uses of surround sound mixing that i don't talk\n",
      "about in this video and now you can also get an entire bonus episode of my podcast cinema of meaning where tom and i\n",
      "discussed 1917 it's available exclusively on our nebula feed and here's the best thing you can get nebula for less than\n",
      "15 for an entire year when you sign up for the curiosity (13:41) stream plus nebula bundle when you sign up for the\n",
      "bundle using my link not only are you getting all the great nebula content i talked about but you also get access to\n",
      "curiosity stream which is a streaming service with thousands of great educational and documentary titles you can get all\n",
      "of this today for 26 percent off when you sign up by going to curiositystream.  #### From this text, tell me about how\n",
      "the sound of rain is used in the movie\n",
      "<agent_01> Generating a response: \n"
     ]
    }
   ],
   "source": [
    "prompt_01 = f\"{data} #### From this text, tell me about how the sound of rain is used in the movie\"\n",
    "\n",
    "wprint(\"<agent_01> Prompt: \"+prompt_01)\n",
    "\n",
    "print(\"<agent_01> Generating a response: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the video transcript, the sound of rain is used in the movie \"The Batman\" to create a sense of realism and\n",
      "atmosphere. The narrator explains that when the rain starts falling during a scene, it's initially audible, but then\n",
      "suddenly stops once gunfire breaks out. To demonstrate this, the narrator provides a modified version of the scene where\n",
      "the sound of the rain remains audible, even after the gunshots start.  This technique is used to create an\n",
      "impressionistic and expressionist sound design, rather than trying to accurately represent reality. The goal is not to\n",
      "make the sound realistic, but to create a certain feeling or atmosphere that enhances the emotional impact of the scene.\n",
      "By leaving the sound of the rain at its original volume while muting the gunfire, the filmmakers are able to achieve\n",
      "this effect and draw the audience's attention to the contrast between the peaceful rain and the intense action.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = ollama.chat(model = 'llama3',\n",
    "                         messages = [{\n",
    "                           \"role\":\"user\",\n",
    "                        #    \"content\":\"tell me about a cool species of frog\"\n",
    "                        \"content\": prompt_01\n",
    "                       }])\n",
    "\n",
    "wprint(response[\"message\"][\"content\"])\n",
    "# print(response)\n",
    "# print(response[\"message\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above feels like a pretty great answer actually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function-ising and further checks\n",
    "\n",
    "I want this to be a bit more re-usable within a function, where I can just specify the data to read from, and the question I want it to answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_read_and_respond(input_file, question, print_prompt_with_data = False):\n",
    "    with open(input_file,'r') as file:\n",
    "        data = file.read()\n",
    "\n",
    "\n",
    "    #debugging statement to confirm file was loaded\n",
    "    if data:\n",
    "        print(\"File loaded successfully\")\n",
    "    else:\n",
    "        print(\"Load in a file\")\n",
    "\n",
    "\n",
    "    prompt_01 = f\"{data} #### From this text, {question}\"\n",
    "\n",
    "    if print_prompt_with_data:\n",
    "        wprint(\"Prompt: \"+prompt_01)\n",
    "\n",
    "    print(\"Generating a response: \")\n",
    "\n",
    "\n",
    "\n",
    "    response = ollama.chat(model = 'llama3',\n",
    "                            messages = [{\n",
    "                            \"role\":\"user\",\n",
    "                            #    \"content\":\"tell me about a cool species of frog\"\n",
    "                            \"content\": prompt_01\n",
    "                        }])\n",
    "\n",
    "    wprint(response[\"message\"][\"content\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully\n",
      "Generating a response: \n",
      "According to the video, in one scene where gunfire breaks out, the sound of rain initially audible suddenly falls away\n",
      "into silence. The narrator suggests that this is not because the rain actually stopped, but rather because the sound\n",
      "designers intentionally muted the sound effect to create a sense of sudden chaos and emphasis on the gunshots.  The\n",
      "narrator also provides a modified version of the scene where the sound of rain remains audible, even after the gunfire\n",
      "starts. This altered version makes the gunshots sound quieter than they would if the rain were actually present in\n",
      "reality. This example illustrates how the sound design in The Batman is not necessarily trying to accurately represent\n",
      "the real world, but rather create a subjective and impressionistic experience for the audience.\n"
     ]
    }
   ],
   "source": [
    "llama_read_and_respond(input_file='data/personal/batman_sound_video_essay.txt', \n",
    "                       question = 'tell me about how the sound of rain is used in the movie')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully\n",
      "Generating a response: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video was sponsored by Curiosity Stream.\n"
     ]
    }
   ],
   "source": [
    "llama_read_and_respond(input_file='data/personal/batman_sound_video_essay.txt', \n",
    "                       question = 'who sponsored the video?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this seems to be working. Now I'd like it to try reading something from my CV, because previously it seemed to be struggling with that. I've just changed the extension from a .tex file to .txt, and I want to see if it can answer basic questions (e.g. about dates of employment). This might be harder for it to do because it's still got all of these latex formatting things in there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully\n",
      "Generating a response: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided LaTeX code, the most recent job listed is:  **NZ Royal Commission Inquiry - COVID-19 Lessons\n",
      "Learned**  As a **Principal Data Analyst**, you worked from May 2024 to July 2024. In this role, you:  * Created high-\n",
      "quality visualisations to support the Inquiry * Conducted analyses and created visualisations to highlight the disparate\n",
      "impact of COVID-19 on Māori and Pacific ethnic groups and people living in areas of higher socioeconomic deprivation *\n",
      "Worked closely with the Chair of the Commission to discuss how to tell the story of the COVID pandemic through the above\n",
      "visualisations in a way that draws out lessons for future pandemics.\n"
     ]
    }
   ],
   "source": [
    "llama_read_and_respond(input_file='data/personal/Nik_Mitchell_CV_2024_07_21.txt', \n",
    "                       question = 'what is the most recent job on that list, and what did I do in that job?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, this is the correct answer, and way better than what I got when I tried to just run ollama in the terminal. But also, it seems to really directly copy-paste exactly what I wrote in my bullet points here (possibly because it was in bullet point form?). Next, asking it to be more concise & summarise a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully\n",
      "Generating a response: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most recent job listed is \"Principal Data Analyst\" at NZ Royal Commission Inquiry - COVID-19 Lessons Learned (May\n",
      "2024 - July 2024).  In this role, my main responsibilities included:  * Creating high-quality visualizations to support\n",
      "the inquiry * Conducting analyses and creating visualizations to highlight pandemic trends in New Zealand and other\n",
      "countries * Working closely with the Chair of the Commission to discuss how to tell the story of the COVID pandemic\n",
      "through visualizations.\n"
     ]
    }
   ],
   "source": [
    "llama_read_and_respond(input_file='data/personal/Nik_Mitchell_CV_2024_07_21.txt', \n",
    "                       question = 'what is the most recent job on that list, and what did I do in that job? Please be concise and summarise the responsibilities rather than copying the whole description')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seemed to work. I'm still curious about this bullet point thing though.\n",
    "\n",
    "#### Bullet point formatting investigation\n",
    "\n",
    "I have a hypothesis that this being in bullet points convinced llama to repeat it more exactly. I have a subquestion in here about whether it's specifically all the tex formatting or whether bullet points in normal text would have the same effect. \n",
    "\n",
    "To investigate this, I'm going to present llama3 with three different versions of that one section of my CV\n",
    "- One with all the latex formatting (baseline)\n",
    "- One with latex formatting removed, but in bullet point format still\n",
    "- One where I've rewritten the information in sentence format\n",
    "\n",
    "and see whether they have different summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully\n",
      "Prompt: \\subsection*{Experience}  \\begin{itemize}   \\parskip=0.1em        \\item   %  \\headerrow   {\\textbf{NZ Royal\n",
      "Commission Inquiry - COVID-19 Lessons Learned}}   %  {\\textbf{Wellington, NZ}}   \\\\   \\headerrow   {\\emph{Principal Data\n",
      "Analyst}}   {\\emph{May 2024 -- July 2024}}   {\\emph{Created high-quality visualisations to support the Inquiry}}\n",
      "\\begin{itemize*}         \\item Created visualisations that contextualised pandemic trends (COVID-19 cases,\n",
      "hospitalisations, deaths and vaccinations) in New Zealand against policy decisions (e.g. lockdowns, border closing) and\n",
      "pandemic trends in other countries         \\item Also conducted analyses and created visualisations to highlight the\n",
      "disparate impact of COVID-19 on M\\a=aori and Pacific ethnic groups and people living in areas of higher socioeconomic\n",
      "deprivation         \\item Worked closely with the Chair of the Commission to discuss how to tell the story of the COVID\n",
      "pandemic through the above visualisations in a way that draws out lessons for future pandemics\n",
      "\\end{itemize*}     #### From this text, what is the title of the job on that list, and what did I do in that job?\n",
      "Generating a response: \n",
      "The title of the job is:  {\\emph{Principal Data Analyst}}  As a Principal Data Analyst, you:  * Created high-quality\n",
      "visualisations to support the Inquiry         + Contextualised pandemic trends (COVID-19 cases, hospitalisations, deaths\n",
      "and vaccinations) in New Zealand against policy decisions (e.g. lockdowns, border closing) and pandemic trends in other\n",
      "countries         + Highlighted the disparate impact of COVID-19 on Māori and Pacific ethnic groups and people living in\n",
      "areas of higher socioeconomic deprivation         + Worked closely with the Chair of the Commission to discuss how to\n",
      "tell the story of the COVID pandemic through visualisations in a way that draws out lessons for future pandemics\n"
     ]
    }
   ],
   "source": [
    "# TeX formatting only\n",
    "llama_read_and_respond(input_file='data/personal/Nik_Mitchell_CV_principal_only_tex_formatting.txt', \n",
    "                       question = 'what is the title of the job on that list, and what did I do in that job?',\n",
    "                       print_prompt_with_data = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully\n",
      "Prompt: Experience  NZ Royal Commission Inquiry - COVID-19 Lessons Learned (Wellington, NZ)  Principal Data Analyst (May\n",
      "2024 -- July 2024) - Created high-quality visualisations to support the Inquiry   - Created visualisations that\n",
      "contextualised pandemic trends (COVID-19 cases, hospitalisations, deaths and vaccinations) in New Zealand against policy\n",
      "decisions (e.g. lockdowns, border closing) and pandemic trends in other countries   - Also conducted analyses and\n",
      "created visualisations to highlight the disparate impact of COVID-19 on M\\a=aori and Pacific ethnic groups and people\n",
      "living in areas of higher socioeconomic deprivation   - Worked closely with the Chair of the Commission to discuss how\n",
      "to tell the story of the COVID pandemic through the above visualisations in a way that draws out lessons for future\n",
      "pandemics #### From this text, what is the title of the job on that list, and what did I do in that job?\n",
      "Generating a response: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the text, the title of the job is \"Principal Data Analyst\". As a Principal Data Analyst, you:  * Created\n",
      "high-quality visualisations to support the NZ Royal Commission Inquiry * Conducted analyses and created visualisations\n",
      "to:         + Contextualise pandemic trends (COVID-19 cases, hospitalisations, deaths, and vaccinations) in New Zealand\n",
      "against policy decisions (e.g. lockdowns, border closures) and pandemic trends in other countries         + Highlight\n",
      "the disparate impact of COVID-19 on Māori and Pacific ethnic groups and people living in areas of higher socioeconomic\n",
      "deprivation * Worked closely with the Chair of the Commission to discuss how to tell the story of the COVID pandemic\n",
      "through visualisations and draw out lessons for future pandemics.\n"
     ]
    }
   ],
   "source": [
    "# bullets but not in tex only\n",
    "llama_read_and_respond(input_file='data/personal/Nik_Mitchell_CV_principal_only_bullets.txt', \n",
    "                       question = 'what is the title of the job on that list, and what did I do in that job?',\n",
    "                       print_prompt_with_data = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully\n",
      "Prompt: Experience  NZ Royal Commission Inquiry - COVID-19 Lessons Learned (Wellington, NZ)  Principal Data Analyst (May\n",
      "2024 -- July 2024) Created high-quality visualisations to support the Inquiry.  Created visualisations that\n",
      "contextualised pandemic trends (COVID-19 cases, hospitalisations, deaths and vaccinations) in  New Zealand against\n",
      "policy decisions (e.g. lockdowns, border closing) and pandemic trends in other countries. Also conducted  analyses and\n",
      "created visualisations to highlight the disparate impact of COVID-19 on M\\a=aori and Pacific ethnic groups and  people\n",
      "living in areas of higher socioeconomic deprivation. Worked closely with the Chair of the Commission to discuss how  to\n",
      "tell the story of the COVID pandemic through the above visualisations in a way that draws out lessons for future\n",
      "pandemics. #### From this text, what is the title of the job on that list, and what did I do in that job?\n",
      "Generating a response: \n",
      "The title of the job is \"Principal Data Analyst\" and according to the text, you:  * Created high-quality visualisations\n",
      "to support the Inquiry * Contextualized pandemic trends (COVID-19 cases, hospitalizations, deaths, and vaccinations) in\n",
      "New Zealand against policy decisions (e.g. lockdowns, border closing) and pandemic trends in other countries. *\n",
      "Conducted analyses and created visualisations to highlight the disparate impact of COVID-19 on Māori and Pacific ethnic\n",
      "groups and people living in areas of higher socioeconomic deprivation. * Worked closely with the Chair of the Commission\n",
      "to discuss how to tell the story of the COVID pandemic through these visualizations in a way that draws out lessons for\n",
      "future pandemics.\n"
     ]
    }
   ],
   "source": [
    "# bullets but not in tex only\n",
    "llama_read_and_respond(input_file='data/personal/personalNik_Mitchell_CV_principal_only_text.txt', \n",
    "                       question = 'what is the title of the job on that list, and what did I do in that job?',\n",
    "                       print_prompt_with_data = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis seems to be falsified by this (bullets-only gave an answer with a bit of a summary at the end, whereas tex formatting and text-only versions stuck to the text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (Retrieval-Augmented Generator)\n",
    "\n",
    "Why would we want to create a RAG? The above seemed to work just fine.\n",
    "\n",
    "I have a suspicion that the issue here is to do with context windows. When making a RAG, we're first going to create a database by chunking up all the inputs into manageable-sized pieces (with overlap between chunks) and then using particular embeddings to encode the meaning of the chunks as vectors. Once we have that, we can use the same embeddings on the input question, and then retrieve the top few chunks that have the most similar meaning vectors (e.g. smallest euclidean distance apart) and use this subset of data to construct the answer from.\n",
    "\n",
    "I suspect that the reason for creating a RAG is this is a context window limitation. The LLM needs to know which information to focus on, so having a method for retrieving the most relevant data allows it to work much more efficiently with a large amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now working through [this video](https://www.youtube.com/watch?v=tcqEUSNCn8I)(about how to make a RAG) - will use OpenAI embeddings here rather than Llama.\n",
    "\n",
    "Has an associated [git repo](https://github.com/pixegami/langchain-rag-tutorial) - might clone this.\n",
    "\n",
    "I've grabbed a version of the Wizard of Oz from the Gutenberg Project website [link](https://www.gutenberg.org/ebooks/55)\n",
    "\n",
    "### Creating the database (based on create_database.py)\n",
    "\n",
    "First loading in a bunch of packages I'll need\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import openai \n",
    "# from dotenv import load_dotenv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Load environment variables. Assumes that project contains .env file with API keys\n",
    "# load_dotenv()\n",
    "#---- Set OpenAI API key \n",
    "# Change environment variable name from \"OPENAI_API_KEY\" to the name given in \n",
    "# your .env file.\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = \"data/wizard_of_oz\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next going to define some functions to use to load the data, split it into chunks, and then turn it into a Chroma database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_documents():\n",
    "    loader = DirectoryLoader(DATA_PATH, glob=\"*.md\")\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "\n",
    "def split_text(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "    document = chunks[10]\n",
    "    print(document.page_content)\n",
    "    print(document.metadata)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def save_to_chroma(chunks: list[Document]):\n",
    "    # Clear out the database first.\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "    # Create a new DB from the documents.\n",
    "    db = Chroma.from_documents(\n",
    "        chunks, OpenAIEmbeddings(), persist_directory=CHROMA_PATH\n",
    "    )\n",
    "    db.persist()\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the functions to create the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 documents into 1127 chunks.\n",
      "Introduction\n",
      "{'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 1870}\n"
     ]
    }
   ],
   "source": [
    "chunks = split_text(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1127 chunks to chroma.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikmitchell/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "save_to_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 documents into 1127 chunks.\n",
      "Introduction\n",
      "{'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 1870}\n",
      "Saved 1127 chunks to chroma.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import openai\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Load environment variables. Assumes that project contains .env file with API keys\n",
    "# Set OpenAI API key\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = \"data/wizard_of_oz\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_data_store():\n",
    "    documents = load_documents()\n",
    "    chunks = split_text(documents)\n",
    "    save_to_chroma(chunks)\n",
    "\n",
    "\n",
    "def load_documents():\n",
    "    loader = DirectoryLoader(DATA_PATH, glob=\"*.md\")\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "\n",
    "def split_text(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "    document = chunks[10]\n",
    "    print(document.page_content)\n",
    "    print(document.metadata)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def save_to_chroma(chunks: list[Document]):\n",
    "    # Clear out the database first.\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "    # Create a new DB from the documents.\n",
    "    db = Chroma.from_documents(\n",
    "        chunks, OllamaEmbeddings(model=\"nomic-embed-text\"), persist_directory=CHROMA_PATH\n",
    "    )\n",
    "    db.persist()\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n",
    "\n",
    "\n",
    "generate_data_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikmitchell/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:784: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 37936}, page_content='“It must be inconvenient to be made of flesh,” said the Scarecrow\\nthoughtfully, “for you must sleep, and eat and drink. However, you have\\nbrains, and it is worth a lot of bother to be able to think properly.”'), -248.2220163170763), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 104484}, page_content='“Why should I do this for you?” asked the Lady.\\n\\n“Because you are wise and powerful, and no one else can help me,”\\nanswered the Scarecrow.'), -249.9570692234033), (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 78361}, page_content='“Nothing that I know of,” answered the Woodman; but the Scarecrow, who\\nhad been trying to think, but could not because his head was stuffed\\nwith straw, said, quickly, “Oh, yes; you can save our friend, the\\nCowardly Lion, who is asleep in the poppy bed.”'), -257.10161416508873)]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 37936}, page_content='“It must be inconvenient to be made of flesh,” said the Scarecrow\\nthoughtfully, “for you must sleep, and eat and drink. However, you have\\nbrains, and it is worth a lot of bother to be able to think properly.”'),\n",
       "  -248.2220163170763),\n",
       " (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 104484}, page_content='“Why should I do this for you?” asked the Lady.\\n\\n“Because you are wise and powerful, and no one else can help me,”\\nanswered the Scarecrow.'),\n",
       "  -249.9570692234033),\n",
       " (Document(metadata={'source': 'data/wizard_of_oz/wizard_of_oz.md', 'start_index': 78361}, page_content='“Nothing that I know of,” answered the Woodman; but the Scarecrow, who\\nhad been trying to think, but could not because his head was stuffed\\nwith straw, said, quickly, “Oh, yes; you can save our friend, the\\nCowardly Lion, who is asleep in the poppy bed.”'),\n",
       "  -257.10161416508873)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = Chroma(persist_directory=CHROMA_PATH, embedding_function=OllamaEmbeddings(model=\"nomic-embed-text\"))\n",
    "db.similarity_search_with_relevance_scores(\"scarecrow needs\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning investigation\n",
    "\n",
    "I'm kinda curious to try to build something a bit more flexible here, and use that to investigate a few questions\n",
    "- Does it make a difference if you use OpenAIEmbeddings() or OllamaEmbeddings()?\n",
    "- Can I build several different chromadbs with different embeddings for different datasets\n",
    "    - Wizard of Oz\n",
    "    - Alice in Wonderland\n",
    "    - My personal files (CV, batman video essay)\n",
    "        - does it matter if I mash these together into a single database, even though they're about totally different things?\n",
    "- can I extend this to read PDF files?\n",
    "\n",
    "I'm a bit worried about doing this if it's not on the mainline to being able to do AI safety work, but I also think that just being curious and following my nose and making functions to output different things and label files and folders appropriately in python etc is going to be valuable.\n",
    "\n",
    "__One thing that I think will be super useful is just getting set up to interact with ChatGPT via API - maybe come back to that after??__\n",
    "\n",
    "Also going to shove all this in git now, because I want to be able to roll it back at some point if I screw it up, and restarting would be a bit of a pain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_store(data_description, embeddings_description):\n",
    "\n",
    "    CHROMA_PATH=os.path.join(\"chroma\",data_description, embeddings_description)\n",
    "    DATA_PATH =os.path.join(\"data\",data_description)\n",
    "\n",
    "    print(f\"Data source: {data_description}, Embeddings: {embeddings_description}\")\n",
    "\n",
    "    # print(f\"CHROMA_PATH is {CHROMA_PATH}\")\n",
    "    # print(f\"DATA_PATH is {DATA_PATH}\")\n",
    "    \n",
    "    if embeddings_description == \"openai_embeddings\":\n",
    "        embedding_function = OpenAIEmbeddings()\n",
    "    elif embeddings_description == \"ollama_embeddings\":\n",
    "        embedding_function = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "\n",
    "\n",
    "    documents = load_documents(data_path=DATA_PATH)\n",
    "    chunks = split_text(documents)\n",
    "    save_to_chroma(chunks, embedding_function, chroma_path= CHROMA_PATH)\n",
    "\n",
    "\n",
    "def load_documents(data_path):\n",
    "    loader = DirectoryLoader(data_path, glob=\"*.md\")\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "\n",
    "def split_text(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "    ## print test example\n",
    "    # document = chunks[10]\n",
    "    # print(document.page_content)\n",
    "    # print(document.metadata)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def save_to_chroma(chunks: list[Document], embedding_function, chroma_path):\n",
    "    # Clear out the database first.\n",
    "    if os.path.exists(chroma_path):\n",
    "        shutil.rmtree(chroma_path)\n",
    "\n",
    "    # Create a new DB from the documents.\n",
    "    db = Chroma.from_documents(\n",
    "        chunks, embedding_function, persist_directory=chroma_path\n",
    "    )\n",
    "    db.persist()\n",
    "    print(f\"Saved {len(chunks)} chunks to {chroma_path}.\")\n",
    "\n",
    "\n",
    "# generate_data_store(data_description       = data_descriptions[0],\n",
    "#                     embeddings_description = embeddings_descriptions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source: wizard_of_oz, Embeddings: openai_embeddings\n",
      "Split 1 documents into 1127 chunks.\n",
      "Saved 1127 chunks to chroma/wizard_of_oz/openai_embeddings.\n",
      "Data source: wizard_of_oz, Embeddings: ollama_embeddings\n",
      "Split 1 documents into 1127 chunks.\n",
      "Saved 1127 chunks to chroma/wizard_of_oz/ollama_embeddings.\n",
      "Data source: alice_in_wonderland, Embeddings: openai_embeddings\n",
      "Split 1 documents into 801 chunks.\n",
      "Saved 801 chunks to chroma/alice_in_wonderland/openai_embeddings.\n",
      "Data source: alice_in_wonderland, Embeddings: ollama_embeddings\n",
      "Split 1 documents into 801 chunks.\n",
      "Saved 801 chunks to chroma/alice_in_wonderland/ollama_embeddings.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "# data_descriptions = [\"wizard_of_oz\",\"alice_in_wonderland\",\"personal\"] ## Commenting out personal for now because it does't use markdown files\n",
    "data_descriptions = [\"wizard_of_oz\",\"alice_in_wonderland\"]\n",
    "embeddings_descriptions = [\"openai_embeddings\",\"ollama_embeddings\"]\n",
    "\n",
    "for data_description, embeddings_description in itertools.product(data_descriptions, embeddings_descriptions):\n",
    "    generate_data_store(data_description, embeddings_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, that works. This is exciting. I should get the question-asking part running up soon too.\n",
    "\n",
    "I'm also curious now about the embeddings, and how they work for ollama versus openai. The compare_embeddings.py file has an interesting example comparing the distance of \"apple\" from \"orange\" vs \"apple\" from \"iphone\". I'd kind of like to have a go at building a list of 5 or 6 different words and calculating the distance from each of them to the others w ollama and open ai. \n",
    "\n",
    "I'm thinking maybe a set of faceted graphs, faceted by word1, and going through all of the word2s and doing bar graphs or somethiing, with different colours for openai vs ollama embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
